{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8952fa6-1c50-43cd-97d1-a9a11efa837e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning in Python \n",
    "## Session 02 - Keras Advanced Concepts\n",
    "\n",
    "- *Course*: Big Data and Language Technologies\n",
    "- *Date*: 11.04.2022\n",
    "\n",
    "This session will cover a few more advanced concepts around Deep Learning in Python with Keras. We will build upon the ideas from the last session and learn about ways to customize the workflow further in detail. We will also learn how to solve some problems that we faced during the last session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30be34-5174-48b0-9ede-2ed49f250c4a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad06a31c-b4a7-49e3-a735-ae187b92cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf66fed-d797-4119-acd3-ee9022f18e9e",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401d3bc-eff8-4ac0-a34b-7c2901a91bfe",
   "metadata": {},
   "source": [
    "This time, we will simply use a wrapper provided by Keras to load up the IMDB dataset that we explored in the last session. For reference, see the [API docs](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d9c708-cb03-4df8-b3a8-a88d289c6d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_FROM=3\n",
    "NUM_WORDS=1000\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=NUM_WORDS,index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870f979-c326-4ebf-8317-50735b1ba874",
   "metadata": {},
   "source": [
    "Note that this already provides us with a train-test split.\n",
    "\n",
    "This dataset is already built using word indices instead of word strings. For transforming text from and to indices using the word index, see [this example](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index#example).\n",
    "\n",
    "**Exercise**: Explore the first 3 samples of X_train by converting them back to strings. What is going wrong? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a4245-93d3-468e-997f-d17d8349c478",
   "metadata": {},
   "source": [
    "### Naive solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5946262-1ae6-49b2-86fc-a875dea500c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: positive\n",
      "the as you with out themselves powerful and and their becomes and had and of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every and and movie except her was several of enough more with is now and film as you of and and unfortunately of you than him that with out themselves her get for was and of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of and and with heart had and they of here that with her serious to have does when from why what have and they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br and to whether from than out themselves history he name half some br of and and was two most of mean for 1 any an and she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her and most that with wasn't to with and acting watch an for with and film want an\n",
      "\n",
      "label: negative\n",
      "the thought and thought and do making to is and and and while he of jack in where and as getting on was did hands fact characters to always life and not as me can't in at are br of sure your way of little it and and to view of love it so and of guy it used and of where it of here and film of and to don't all unique some like of direction it if out her and and keep of and he and to makes this and and of and it thought begins br and and budget and though ok and and for ever better were and and for budget look and any to of making it out and and for effects show to show cast this family us scenes more it and making and to and finds tv and to of and these thing wants but and an and and as it is video do you david see and it in few those are of and for with of and to one is very work dark they don't do dvd with those them\n",
      "\n",
      "label: negative\n",
      "the as there in at by br of sure many br of and no only women was than doesn't as you never of and night that with and they bad out and plays of how star so stories film comes and and of and they don't do that had with of hollywood br of my seeing fan this of and out body shots in having because and it's and and first were and for from look and sense from me and die in character as and and but is you that isn't one song just is him less are and not are you that different just even by this of you there is and when it part are film's love film's and was big also light don't and as it in character looked cinematography so stories is far br man acting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "inverted_word_index = {v:k for k,v in word_index.items()}\n",
    "\n",
    "for i in range(3):\n",
    "    decoded_sequence = \" \".join(inverted_word_index[ind] for ind in X_train[i])\n",
    "    print(\"label:\",[\"negative\",\"positive\"][y_train[i]])\n",
    "    print(decoded_sequence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565a0a1-881d-4ea6-9032-5f487592d65c",
   "metadata": {},
   "source": [
    "### Better solution that takes into account the special tokens that we defined implicitly while loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095a5c2f-ac60-48f3-b0d5-78d80d8658a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: positive\n",
      "<START> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you <UNK> at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> <UNK> i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so <UNK> because it was true and was <UNK> life after all that was <UNK> with us all\n",
      "\n",
      "label: negative\n",
      "<START> big <UNK> big <UNK> bad music and a <UNK> <UNK> <UNK> these are the words to best <UNK> this terrible movie i love cheesy horror movies and i've seen <UNK> but this had got to be on of the worst ever made the plot is <UNK> <UNK> and ridiculous the acting is an <UNK> the script is completely <UNK> the best is the end <UNK> with the <UNK> and how he worked out who the killer is it's just so <UNK> <UNK> written the <UNK> are <UNK> and funny in <UNK> <UNK> the <UNK> is big lots of <UNK> <UNK> men <UNK> those cut <UNK> <UNK> that show off their <UNK> <UNK> that men actually <UNK> them and the music is just <UNK> <UNK> that plays over and over again in almost every scene there is <UNK> music <UNK> and <UNK> taking away <UNK> and the <UNK> still doesn't close for <UNK> all <UNK> <UNK> this is a truly bad film whose only <UNK> is to look back on the <UNK> that was the <UNK> and have a good old laugh at how bad everything was back then\n",
      "\n",
      "label: negative\n",
      "<START> this has to be one of the worst films of the <UNK> when my friends i were watching this film being the <UNK> audience it was <UNK> at we just <UNK> watched the first half an hour with our <UNK> <UNK> the <UNK> at how bad it really was the rest of the time everyone else in the <UNK> just started talking to each other <UNK> or <UNK> <UNK> into their <UNK> that they actually <UNK> money they had <UNK> working to watch this <UNK> <UNK> for a film it must have looked like a great idea on <UNK> but on film it looks like no one in the film has a <UNK> what is going on crap acting crap <UNK> i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "inverted_word_index = {v+INDEX_FROM:k for k,v in word_index.items()}\n",
    "inverted_word_index |= {0:\"<PAD>\",\n",
    "             1: \"<START>\",\n",
    "              2:\"<UNK>\",\n",
    "            3:  \"<UNUSED>\"}\n",
    "\n",
    "for i in range(3):\n",
    "    decoded_sequence = \" \".join(inverted_word_index[ind] for ind in X_train[i])\n",
    "    print(\"label:\",[\"negative\",\"positive\"][y_train[i]])\n",
    "    print(decoded_sequence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48996642-b9e9-461c-9a89-cc05ff3cdabc",
   "metadata": {},
   "source": [
    "## `tf.data.Dataset`\n",
    "\n",
    "Using `tf.data.Dataset`, we can represent very large datasets (will become very important later in the semester). Tensorflow will handle many features necessary for that internally.\n",
    "\n",
    "**Exercise**: Use `tf.data.Dataset.from_generator` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)) to convert our ndarray-based dataset to `tf.data.Dataset`. Provide an `output_signature=(X,y)` (you will also have to have the generator return this format).\n",
    "\n",
    "Using `tf.data.Dataset.from_tensor_slices` is probably difficult because the data is not padded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fff98c6-a2c4-4af4-9b00-8af744fff214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(Xs, ys):\n",
    "    for X,y in zip(Xs,ys):\n",
    "        yield (X,y)\n",
    "\n",
    "output_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),tf.TensorSpec(shape=[],dtype=tf.int32))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(lambda: gen(X_train,y_train), output_signature=output_signature)\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: gen(X_test,y_test), output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68343ea1-872a-4d37-b60d-74cb10bf12fe",
   "metadata": {},
   "source": [
    "Converting the data back to numpy is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861d5b4f-4212-4286-924f-83e20f687be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
       "          2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
       "          2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
       "        385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
       "         50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
       "         22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
       "         22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
       "          8, 316,   8, 106,   5,   4,   2,   2,  16, 480,  66,   2,  33,\n",
       "          4, 130,  12,  16,  38, 619,   5,  25, 124,  51,  36, 135,  48,\n",
       "         25,   2,  33,   6,  22,  12, 215,  28,  77,  52,   5,  14, 407,\n",
       "         16,  82,   2,   8,   4, 107, 117,   2,  15, 256,   4,   2,   7,\n",
       "          2,   5, 723,  36,  71,  43, 530, 476,  26, 400, 317,  46,   7,\n",
       "          4,   2,   2,  13, 104,  88,   4, 381,  15, 297,  98,  32,   2,\n",
       "         56,  26, 141,   6, 194,   2,  18,   4, 226,  22,  21, 134, 476,\n",
       "         26, 480,   5, 144,  30,   2,  18,  51,  36,  28, 224,  92,  25,\n",
       "        104,   4, 226,  65,  16,  38,   2,  88,  12,  16, 283,   5,  16,\n",
       "          2, 113, 103,  32,  15,  16,   2,  19, 178,  32]),\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285e620-2c1a-4f91-ab5c-3b44e7a2bece",
   "metadata": {},
   "source": [
    "## Dataset persistence\n",
    "\n",
    "Tensorflow makes it quite easy to save and load `tf.data.Dataset`.\n",
    "\n",
    "### Using `tf.data.experimental.save` and `load`\n",
    "\n",
    "`tf.data.experimental.save` ([docs](https://tensorflow.google.cn/api_docs/python/tf/data/experimental/save)) and `load` ([docs](https://tensorflow.google.cn/api_docs/python/tf/data/experimental/load)) can be used to persist a Dataset to storage. This will create multiple files (shards).\n",
    "\n",
    "**Exercise**: Save and load our dataset to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e9709d-b630-4ad7-8638-134609b4efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(train_ds,path=\"imdb_train\")\n",
    "tf.data.experimental.save(test_ds,path=\"imdb_test\")\n",
    "\n",
    "train_ds=tf.data.experimental.load(path=\"imdb_train\")\n",
    "test_ds=tf.data.experimental.load(path=\"imdb_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b32bbd-3cc9-4c52-aa24-e21b266c4c12",
   "metadata": {},
   "source": [
    "Let's test it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f1ece19-4531-4afd-9a1b-582d23bc6bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
       "          2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
       "          2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
       "        385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
       "         50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
       "         22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
       "         22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
       "          8, 316,   8, 106,   5,   4,   2,   2,  16, 480,  66,   2,  33,\n",
       "          4, 130,  12,  16,  38, 619,   5,  25, 124,  51,  36, 135,  48,\n",
       "         25,   2,  33,   6,  22,  12, 215,  28,  77,  52,   5,  14, 407,\n",
       "         16,  82,   2,   8,   4, 107, 117,   2,  15, 256,   4,   2,   7,\n",
       "          2,   5, 723,  36,  71,  43, 530, 476,  26, 400, 317,  46,   7,\n",
       "          4,   2,   2,  13, 104,  88,   4, 381,  15, 297,  98,  32,   2,\n",
       "         56,  26, 141,   6, 194,   2,  18,   4, 226,  22,  21, 134, 476,\n",
       "         26, 480,   5, 144,  30,   2,  18,  51,  36,  28, 224,  92,  25,\n",
       "        104,   4, 226,  65,  16,  38,   2,  88,  12,  16, 283,   5,  16,\n",
       "          2, 113, 103,  32,  15,  16,   2,  19, 178,  32]),\n",
       " 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569e803-4576-4ba7-bfeb-f3425f3c3d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: The [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord) is the traditional method to save serialized data, which might save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ff490-103c-4273-8fed-31a48db4ce18",
   "metadata": {},
   "source": [
    "## `map` and `filter`\n",
    "\n",
    "Using `map` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)) and `filter` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter)) on `tf.data.Dataset` is very convenient, as the used functions are applied on the fly, controlled by demand.\n",
    "\n",
    "It is recommended to use the `tf.function` decorator ([docs](https://www.tensorflow.org/api_docs/python/tf/function)) to improve performance if possible.\n",
    "\n",
    "**Exercise**: From the `train_ds`, filter out all reviews shorter than 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44557466-4ad8-49c4-9654-6daccdf230e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def filter_func(X,y):\n",
    "    return tf.shape(X)[0]>=100\n",
    "\n",
    "train_ds = train_ds.filter(filter_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaaa172-4d42-444b-8d2d-43a455871bf5",
   "metadata": {},
   "source": [
    "### \\* Bonus: `flat_map`\n",
    "\n",
    "`map` allows us to modify Dataset samples 1-to-1. If we want to split certain samples into a varying number of samples, we can use `flat_map` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map)).\n",
    "\n",
    "**Exercise**: Use `flat_map` on `train_ds` to split up long reviews into reviews of 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620912d6-5159-4752-8b95-a6c9ec4d7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def map_func(X,y):\n",
    "    size=tf.shape(X)[0]\n",
    "    r=tf.range(0,(size//100)*100,1)\n",
    "    r=tf.reshape(r,[size//100,100])\n",
    "    Xs=tf.gather(X,r)\n",
    "    ys=tf.repeat(y,size//100)\n",
    "    return tf.data.Dataset.from_tensor_slices((Xs,ys))\n",
    "\n",
    "train_ds = train_ds.flat_map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f363dd4d-fd2c-473c-8256-e43d7ed5372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
      "         2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
      "         2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
      "       385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
      "        50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
      "        22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
      "        22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
      "         8, 316,   8, 106,   5,   4,   2,   2,  16]), 1)\n",
      "(array([480,  66,   2,  33,   4, 130,  12,  16,  38, 619,   5,  25, 124,\n",
      "        51,  36, 135,  48,  25,   2,  33,   6,  22,  12, 215,  28,  77,\n",
      "        52,   5,  14, 407,  16,  82,   2,   8,   4, 107, 117,   2,  15,\n",
      "       256,   4,   2,   7,   2,   5, 723,  36,  71,  43, 530, 476,  26,\n",
      "       400, 317,  46,   7,   4,   2,   2,  13, 104,  88,   4, 381,  15,\n",
      "       297,  98,  32,   2,  56,  26, 141,   6, 194,   2,  18,   4, 226,\n",
      "        22,  21, 134, 476,  26, 480,   5, 144,  30,   2,  18,  51,  36,\n",
      "        28, 224,  92,  25, 104,   4, 226,  65,  16]), 1)\n",
      "(array([  1, 194,   2, 194,   2,  78, 228,   5,   6,   2,   2,   2, 134,\n",
      "        26,   4, 715,   8, 118,   2,  14, 394,  20,  13, 119, 954, 189,\n",
      "       102,   5, 207, 110,   2,  21,  14,  69, 188,   8,  30,  23,   7,\n",
      "         4, 249, 126,  93,   4, 114,   9,   2,   2,   5, 647,   4, 116,\n",
      "         9,  35,   2,   4, 229,   9, 340,   2,   4, 118,   9,   4, 130,\n",
      "         2,  19,   4,   2,   5,  89,  29, 952,  46,  37,   4, 455,   9,\n",
      "        45,  43,  38,   2,   2, 398,   4,   2,  26,   2,   5, 163,  11,\n",
      "         2,   2,   4,   2,   9, 194, 775,   7,   2]), 0)\n"
     ]
    }
   ],
   "source": [
    "it=train_ds.as_numpy_iterator()\n",
    "for i in range(3):\n",
    "    print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e672b-80d9-4103-9f8d-fbd245c86847",
   "metadata": {},
   "source": [
    "## Batch, shuffle, repeat\n",
    "\n",
    "In order to make our dataset usable for training, we will need to batch it (split it up into batches), repeat it (so you can train on multiple epochs) and shuffle it (to avoid using the same order every time).\n",
    "\n",
    "In this task, you will learn that the order of these operations indeed matters!\n",
    "\n",
    "Let's create a dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6307bf7-47ae-43e8-bae7-6fef3a3e250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_DS_SIZE=30\n",
    "dummy_ds=tf.data.Dataset.range(DUMMY_DS_SIZE)\n",
    "DUMMY_BATCHSIZE=10\n",
    "DUMMY_BUFFERSIZE=2*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1218b2-ac38-4a89-b860-25d6e4c0067b",
   "metadata": {},
   "source": [
    "**Exercise**: Roll the dice to determine the order in which you will implement shuffle, batch and repeat. Try to spot flaws in the results by inspecting 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e36321-e6d8-4ee4-bbe5-303a9318c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat, shuffle, batch\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice([\"Shuffle, repeat, batch\",\n",
    "                       \"Repeat, shuffle, batch\",\n",
    "                       \"Batch, shuffle, repeat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a0457d-f207-4d71-9bb6-abdffa075889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  0 21  7 22  5 15 27 28]\n",
      "[ 9 14 24  6 10 13 29  1 26 17]\n",
      "[19 18 20  8 25 23 16 11  2 12]\n",
      "\n",
      "[10 16  6 21  7 12  3 20 11 26]\n",
      "[29 24  1 13  0  4  5  8  9 25]\n",
      "[27 14 17 15 28 22  2 23 19 18]\n",
      "\n",
      "[16 10  9 12 17 11 19  6  8  5]\n",
      "[13  7 27 29 18 14 22 28  1  2]\n",
      "[23 21 25 26 15 20  0  3 24  4]\n",
      "\n",
      "[19 11  8 16 10  7 22 17 12 23]\n",
      "[15  2 25  4  9  3 13 24 14  0]\n",
      "[ 5 26  1 18 21 20 28 29  6 27]\n",
      "\n",
      "[ 8  0 19  7 16 22  5 24 12 13]\n",
      "[25  4  6 17 29 23 26 18 21  3]\n",
      "[ 1  2 20 11 10 27 14 15  9 28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dummy_ds=dummy_ds.shuffle(DUMMY_BUFFERSIZE).repeat().batch(DUMMY_BATCHSIZE)\n",
    "for epoch in range(5):\n",
    "    for batch in new_dummy_ds.take(DUMMY_DS_SIZE//DUMMY_BATCHSIZE).as_numpy_iterator():\n",
    "        print(batch)\n",
    "    print()\n",
    "# Observation: Despite being a bit unintuitive (repeating after shuffling?!), this is indeed the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc61020-e1e0-4c5c-9b9d-8950e9968ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  2 20  3 15 16 19 13 10 12]\n",
      "[ 5  9 17 29  0  8  0  4  7 28]\n",
      "[ 6  6  5  1  7 26 24 27 22 12]\n",
      "\n",
      "[ 6  9  7 19  4  1 21 16 14 11]\n",
      "[17 13 20  5  2  0 29  3  1  8]\n",
      "[ 9  3 11  7 10 22 14 15 23  0]\n",
      "\n",
      "[17  7 11  9 21  2 25  6  4 20]\n",
      "[13 27 14 18 19 15  5  4 24  6]\n",
      "[22 29 26  0 10  5  7  2 12 14]\n",
      "\n",
      "[16 14  0  4  5 18  7  1 25  2]\n",
      "[ 8 13  9 12 17  0 23  3 11 22]\n",
      "[ 6 10  8 24 11 19 29  3 27  5]\n",
      "\n",
      "[ 0 13  7  4  2 19 12 18  1 17]\n",
      "[ 6  3 25 24 29 14  5  9  8  7]\n",
      "[28  3 16 26 10  4 20 11  0  5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dummy_ds=dummy_ds.repeat().shuffle(DUMMY_BUFFERSIZE).batch(DUMMY_BATCHSIZE)\n",
    "for epoch in range(5):\n",
    "    for batch in new_dummy_ds.take(DUMMY_DS_SIZE//DUMMY_BATCHSIZE).as_numpy_iterator():\n",
    "        print(batch)\n",
    "    print()\n",
    "# Observation: Some samples occur multiple times inside the epoch (and sometimes even inside a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d648ac7-cf88-4c36-809e-976084e25577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dummy_ds=dummy_ds.batch(DUMMY_BATCHSIZE).shuffle(DUMMY_BUFFERSIZE).repeat()\n",
    "for epoch in range(5):\n",
    "    for batch in new_dummy_ds.take(DUMMY_DS_SIZE//DUMMY_BATCHSIZE).as_numpy_iterator():\n",
    "        print(batch)\n",
    "    print()\n",
    "# Observation: Only shuffles batches -> batch should be called last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623561d-39f8-4ad9-85ca-37ab8e7eb924",
   "metadata": {},
   "source": [
    "### Applying what we found out\n",
    "**Exercise**: Shuffle, repeat and batch (using `padded_batch`) our `train_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b4106f-08d5-45f0-be18-eb9829320d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE=64\n",
    "BUFFERSIZE=2*64\n",
    "train_ds=train_ds.shuffle(BUFFERSIZE).repeat().padded_batch(BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b60b82-c345-4464-8fb2-005d12c304f3",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "\n",
    "Keras allows you to define custom layers. This is useful for:\n",
    "1. Combining multiple pre-defined layers into a single custom layer\n",
    "2. Defining the layer weights explicitly\n",
    "3. Modifying gradients\n",
    "\n",
    "### \"Custom\" dense layer\n",
    "\n",
    "**Exercise**: Re-implement a dense layer using a subclass of the `tf.keras.layers.Layer` class ([docs](https://keras.io/api/layers/base_layer/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a55e5d-d0da-4937-9d2a-b1ccc8b97463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, units=32):\n",
    "      super().__init__()\n",
    "      self.units = units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    w_init = tf.random_normal_initializer()\n",
    "    self.w = tf.Variable(\n",
    "        initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                             dtype='float32'),\n",
    "        trainable=True)\n",
    "    b_init = tf.zeros_initializer()\n",
    "    self.b = tf.Variable(\n",
    "        initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "        trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88704dda-7572-482e-955b-0f549811fc3e",
   "metadata": {},
   "source": [
    "### \\* Bonus: \"Custom\" dropout layer\n",
    "\n",
    "**Exercise**: Re-implement a dropout layer using a subclass of the `Layer` class ([docs](https://keras.io/api/layers/base_layer/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28be0a5d-9eea-4353-a466-b15693a749f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropoutLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, rate):\n",
    "      super().__init__()\n",
    "      self.rate = rate\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    #if training is None:\n",
    "    #    training = tf.keras.backend.learning_phase()\n",
    "    random=tf.where(tf.random.uniform(inputs.shape,minval=0,maxval=1,dtype=tf.float32)<self.rate,0.0,1.0)\n",
    "    dropped_out=inputs*random\n",
    "    dropped_out=dropped_out/(1.0 - self.rate) # sum over all inputs should be unchanged\n",
    "    return tf.where(training,dropped_out,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9a060-bbbb-4423-aeb2-08f7a2681b04",
   "metadata": {},
   "source": [
    "If you want to dive deeper into defining custom layers, see [this guide](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465699c3-1d63-4419-9cd7-4bb36419533e",
   "metadata": {},
   "source": [
    "## Custom Loss\n",
    "\n",
    "Sometimes we are not fully happy with the predifined losses provided by Tensorflow/Keras. See the [docs](https://keras.io/api/losses/#creating-custom-losses) for how to create custom losses based on `y_true` and `y_pred`.\n",
    "\n",
    "**Exercise**: Define a custom loss that computes a weighted crossentropy to rebalance the classes (label `0` and `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4272f41-bd7c-4de7-ba2a-00f4751deaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_sum(tf.where(y_true==tf.ones_like(y_true),\n",
    "                           -tf.math.log(y_pred)*(tf.cast(tf.size(y_true)/tf.reduce_sum(y_true),tf.float32)),\n",
    "                           -tf.math.log(1.0-y_pred)*(tf.cast(tf.size(y_true)/tf.reduce_sum(1-y_true),tf.float32))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d30995-f3b0-4968-b341-2a2e4c8a0202",
   "metadata": {},
   "source": [
    "### The `add_loss()` API\n",
    "\n",
    "Regularization losses are not just based on a comparison of `y_true` and `y_pred`. The `add_loss()` API allows to use layer weights in loss computation. See the [docs](https://keras.io/api/losses/#the-addloss-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece2f76-1112-4e26-9173-f903044e3e6d",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "\n",
    "By defining a subclass to `tf.keras.Model`, we can customize what is happening during `fit()` on a more fine-grained level than using callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c345405-bf49-4be9-9a8c-b8eabb410a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a9f71-20be-4c3d-bc8c-cc0c9f479dbc",
   "metadata": {},
   "source": [
    "Further details on customizing the behavior of `fit()` can be found in [this guide](https://keras.io/guides/customizing_what_happens_in_fit/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38e021-5eef-4713-b539-a266c3fa50ff",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard is a browser application that allows you to supervise the training progress. To access the generated logs, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650df245-7dc7-426a-921f-f7604b21b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12512), started 0:05:55 ago. (Use '!kill 12512' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-813e0f38829f9b61\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-813e0f38829f9b61\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n",
    "# alternatively: !tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b174332-531a-48b3-9cd8-c41b7328229f",
   "metadata": {},
   "source": [
    "We use a special callback to generate the data that TensorBoard will be visualizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe146d7f-ce73-4303-b591-c85714dff511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\",update_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe593a1-16c8-4635-ae96-61e512107a2f",
   "metadata": {},
   "source": [
    "## Assembling everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b8c1d-38df-4689-89c5-e82a9d7d649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,),batch_size=BATCHSIZE)\n",
    "x = tf.keras.layers.Embedding(len(inverted_word_index), 16)(inputs)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = CustomDenseLayer(16)(x)\n",
    "x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "x = CustomDropoutLayer(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "model = CustomModel(inputs,outputs)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=custom_loss)\n",
    "model.fit(train_ds,validation_data=test_ds.batch(1),callbacks=[tensorboard_callback],epochs=10,steps_per_epoch=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
