{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8952fa6-1c50-43cd-97d1-a9a11efa837e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning in Python \n",
    "## Session 02 - Keras Advanced Concepts\n",
    "\n",
    "- *Course*: Big Data and Language Technologies\n",
    "- *Date*: 11.04.2022\n",
    "\n",
    "This session will cover a few more advanced concepts around Deep Learning in Python with Keras. We will build upon the ideas from the last session and learn about ways to customize the workflow further in detail. We will also learn how to solve some problems that we faced during the last session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30be34-5174-48b0-9ede-2ed49f250c4a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad06a31c-b4a7-49e3-a735-ae187b92cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf66fed-d797-4119-acd3-ee9022f18e9e",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401d3bc-eff8-4ac0-a34b-7c2901a91bfe",
   "metadata": {},
   "source": [
    "This time, we will simply use a wrapper provided by Keras to load up the IMDB dataset that we explored in the last session. For reference, see the [API docs](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d9c708-cb03-4df8-b3a8-a88d289c6d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_FROM=3\n",
    "NUM_WORDS=1000\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=NUM_WORDS,index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870f979-c326-4ebf-8317-50735b1ba874",
   "metadata": {},
   "source": [
    "Note that this already provides us with a train-test split.\n",
    "\n",
    "This dataset is already built using word indices instead of word strings. For transforming text from and to indices using the word index, see [this example](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index#example).\n",
    "\n",
    "**Exercise**: Explore the first 3 samples of X_train by converting them back to strings. What is going wrong? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7ad99-5ffd-495f-8bc3-37bf9f371730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48996642-b9e9-461c-9a89-cc05ff3cdabc",
   "metadata": {},
   "source": [
    "## `tf.data.Dataset`\n",
    "\n",
    "Using `tf.data.Dataset`, we can represent very large datasets (will become very important later in the semester). Tensorflow will handle many features necessary for that internally.\n",
    "\n",
    "**Exercise**: Use `tf.data.Dataset.from_generator` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)) to convert our ndarray-based dataset to `tf.data.Dataset`. Provide an `output_signature=(X,y)` (you will also have to have the generator return this format).\n",
    "\n",
    "Using `tf.data.Dataset.from_tensor_slices` is probably difficult because the data is not padded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fff98c6-a2c4-4af4-9b00-8af744fff214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68343ea1-872a-4d37-b60d-74cb10bf12fe",
   "metadata": {},
   "source": [
    "Converting the data back to numpy is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d5b4f-4212-4286-924f-83e20f687be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285e620-2c1a-4f91-ab5c-3b44e7a2bece",
   "metadata": {},
   "source": [
    "## Dataset persistence\n",
    "\n",
    "Tensorflow makes it quite easy to save and load `tf.data.Dataset`.\n",
    "\n",
    "### Using `tf.data.experimental.save` and `load`\n",
    "\n",
    "`tf.data.experimental.save` ([docs](https://tensorflow.google.cn/api_docs/python/tf/data/experimental/save)) and `load` ([docs](https://tensorflow.google.cn/api_docs/python/tf/data/experimental/load)) can be used to persist a Dataset to storage. This will create multiple files (shards).\n",
    "\n",
    "**Exercise**: Save and load our dataset to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e9709d-b630-4ad7-8638-134609b4efca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b32bbd-3cc9-4c52-aa24-e21b266c4c12",
   "metadata": {},
   "source": [
    "Let's test it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ece19-4531-4afd-9a1b-582d23bc6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569e803-4576-4ba7-bfeb-f3425f3c3d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: The [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord) is the traditional method to save serialized data, which might save memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ff490-103c-4273-8fed-31a48db4ce18",
   "metadata": {},
   "source": [
    "## `map` and `filter`\n",
    "\n",
    "Using `map` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)) and `filter` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter)) on `tf.data.Dataset` is very convenient, as the used functions are applied on the fly, controlled by demand.\n",
    "\n",
    "It is recommended to use the `tf.function` decorator ([docs](https://www.tensorflow.org/api_docs/python/tf/function)) to improve performance if possible.\n",
    "\n",
    "**Exercise**: From the `train_ds`, filter out all reviews shorter than 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44557466-4ad8-49c4-9654-6daccdf230e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaaaa172-4d42-444b-8d2d-43a455871bf5",
   "metadata": {},
   "source": [
    "### \\* Bonus: `flat_map`\n",
    "\n",
    "`map` allows us to modify Dataset samples 1-to-1. If we want to split certain samples into a varying number of samples, we can use `flat_map` ([docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map)).\n",
    "\n",
    "**Exercise**: Use `flat_map` on `train_ds` to split up long reviews into reviews of 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620912d6-5159-4752-8b95-a6c9ec4d7538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363dd4d-fd2c-473c-8256-e43d7ed5372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "it=train_ds.as_numpy_iterator()\n",
    "for i in range(3):\n",
    "    print(next(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e672b-80d9-4103-9f8d-fbd245c86847",
   "metadata": {},
   "source": [
    "## Batch, shuffle, repeat\n",
    "\n",
    "In order to make our dataset usable for training, we will need to batch it (split it up into batches), repeat it (so you can train on multiple epochs) and shuffle it (to avoid using the same order every time).\n",
    "\n",
    "In this task, you will learn that the order of these operations indeed matters!\n",
    "\n",
    "Let's create a dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6307bf7-47ae-43e8-bae7-6fef3a3e250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_DS_SIZE=30\n",
    "dummy_ds=tf.data.Dataset.range(DUMMY_DS_SIZE)\n",
    "DUMMY_BATCHSIZE=10\n",
    "DUMMY_BUFFERSIZE=2*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1218b2-ac38-4a89-b860-25d6e4c0067b",
   "metadata": {},
   "source": [
    "**Exercise**: Roll the dice to determine the order in which you will implement shuffle, batch and repeat. Try to spot flaws in the results by inspecting 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e36321-e6d8-4ee4-bbe5-303a9318c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat, shuffle, batch\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice([\"Shuffle, repeat, batch\",\n",
    "                       \"Repeat, shuffle, batch\",\n",
    "                       \"Batch, shuffle, repeat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0457d-f207-4d71-9bb6-abdffa075889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4623561d-39f8-4ad9-85ca-37ab8e7eb924",
   "metadata": {},
   "source": [
    "### Applying what we found out\n",
    "**Exercise**: Shuffle, repeat and batch (using `padded_batch`) our `train_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b4106f-08d5-45f0-be18-eb9829320d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE=64\n",
    "BUFFERSIZE=2*64\n",
    "train_ds=...#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b60b82-c345-4464-8fb2-005d12c304f3",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "\n",
    "Keras allows you to define custom layers. This is useful for:\n",
    "1. Combining multiple pre-defined layers into a single custom layer\n",
    "2. Defining the layer weights explicitly\n",
    "3. Modifying gradients\n",
    "\n",
    "### \"Custom\" dense layer\n",
    "\n",
    "**Exercise**: Re-implement a dense layer using a subclass of the `tf.keras.layers.Layer` class ([docs](https://keras.io/api/layers/base_layer/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a55e5d-d0da-4937-9d2a-b1ccc8b97463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88704dda-7572-482e-955b-0f549811fc3e",
   "metadata": {},
   "source": [
    "### \\* Bonus: \"Custom\" dropout layer\n",
    "\n",
    "**Exercise**: Re-implement a dropout layer using a subclass of the `Layer` class ([docs](https://keras.io/api/layers/base_layer/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28be0a5d-9eea-4353-a466-b15693a749f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c9a060-bbbb-4423-aeb2-08f7a2681b04",
   "metadata": {},
   "source": [
    "If you want to dive deeper into defining custom layers, see [this guide](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465699c3-1d63-4419-9cd7-4bb36419533e",
   "metadata": {},
   "source": [
    "## Custom Loss\n",
    "\n",
    "Sometimes we are not fully happy with the predifined losses provided by Tensorflow/Keras. See the [docs](https://keras.io/api/losses/#creating-custom-losses) for how to create custom losses based on `y_true` and `y_pred`.\n",
    "\n",
    "**Exercise**: Define a custom loss that computes a weighted crossentropy to rebalance the classes (label `0` and `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4272f41-bd7c-4de7-ba2a-00f4751deaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d30995-f3b0-4968-b341-2a2e4c8a0202",
   "metadata": {},
   "source": [
    "### The `add_loss()` API\n",
    "\n",
    "Regularization losses are not just based on a comparison of `y_true` and `y_pred`. The `add_loss()` API allows to use layer weights in loss computation. See the [docs](https://keras.io/api/losses/#the-addloss-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece2f76-1112-4e26-9173-f903044e3e6d",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "\n",
    "By defining a subclass to `tf.keras.Model`, we can customize what is happening during `fit()` on a more fine-grained level than using callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c345405-bf49-4be9-9a8c-b8eabb410a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b7a9f71-20be-4c3d-bc8c-cc0c9f479dbc",
   "metadata": {},
   "source": [
    "Further details on customizing the behavior of `fit()` can be found in [this guide](https://keras.io/guides/customizing_what_happens_in_fit/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38e021-5eef-4713-b539-a266c3fa50ff",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard is a browser application that allows you to supervise the training progress. To access the generated logs, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650df245-7dc7-426a-921f-f7604b21b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n",
    "# alternatively: !tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b174332-531a-48b3-9cd8-c41b7328229f",
   "metadata": {},
   "source": [
    "We use a special callback to generate the data that TensorBoard will be visualizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe146d7f-ce73-4303-b591-c85714dff511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\",update_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe593a1-16c8-4635-ae96-61e512107a2f",
   "metadata": {},
   "source": [
    "## Assembling everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b8c1d-38df-4689-89c5-e82a9d7d649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
