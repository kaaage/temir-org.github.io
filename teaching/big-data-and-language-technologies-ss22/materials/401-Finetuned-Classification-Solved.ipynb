{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e79966b-da5e-4ba7-b88d-68d277bb3ded",
   "metadata": {},
   "source": [
    "# Deep Learning on SLURM\n",
    "\n",
    "## Session 01 - Preparations\n",
    "\n",
    "- *Course*: Big Data and Language Technologies\n",
    "- *Date*: 02.05.2022\n",
    "\n",
    "In this session, we will prepare running a Deep Learning model on our cluster servers running SLURM. First, we will introduce the dataset and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0110e85-a84e-461f-a086-7a87804a8c6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install tensorflow\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac17d1-7b87-4884-a3db-4a411b0b4057",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d1ae21-8b1a-40bf-9b33-4291a339e772",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"model\": \"distilbert-base-uncased\",\n",
    "    \"seq_length\": 512,\n",
    "    \"num_classes\": 20,\n",
    "    \"batch_size\": 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1988f5f6-ff79-418e-ad00-cf63a522ee8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f995b-b8f8-4947-93e5-3851ff2a3c33",
   "metadata": {},
   "source": [
    "## Obtaining Data\n",
    "\n",
    "**Exercise**: load the 20NG datasets using the `sklearn.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feff538d-0922-46f2-a40b-390d8ed7031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "raw_data = fetch_20newsgroups(\n",
    "                data_home=\"./\",\n",
    "                remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "                subset=\"all\"\n",
    "            )\n",
    "\n",
    "class_mapping = dict(enumerate(raw_data[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e337dd3-26d2-4c88-b4fb-5b15c4e5214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows.misc', 3: 'comp.sys.ibm.pc.hardware', 4: 'comp.sys.mac.hardware', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n",
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print(class_mapping)\n",
    "print(raw_data.keys())\n",
    "print(raw_data[\"data\"][0])\n",
    "print(class_mapping[raw_data[\"target\"][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8ac00-cf99-4e3e-923d-5293e2b7a55d",
   "metadata": {},
   "source": [
    "**Exercise**: split the data into training, validation, and test data. (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6dbd9d-4cba-4225-a87b-d46923d9c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = raw_data[\"data\"]\n",
    "y = raw_data[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 1/9, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48616e7f-b151-42b2-a82a-b5ffdcea064b",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**Exercise**: Load the `DistilBertTokenizer` pretrained tokenizer and apply it to tokenize the 20NG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7246f73-80f3-427d-9a99-a2273b43d17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/bdlt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 13:57:58.865870: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-02 13:57:58.865989: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CONFIG[\"model\"])\n",
    "\n",
    "tokenizer_config = {\n",
    "    \"return_tensors\": \"tf\",\n",
    "    \"return_attention_mask\": True,\n",
    "    \"return_token_type_ids\": True,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"truncation\": True,\n",
    "    \"max_length\": CONFIG[\"seq_length\"],\n",
    "}\n",
    "\n",
    "X_train_tokenized = tokenizer.batch_encode_plus(X_train, **tokenizer_config).data\n",
    "X_val_tokenized = tokenizer.batch_encode_plus(X_val, **tokenizer_config).data\n",
    "X_test_tokenized =  tokenizer.batch_encode_plus(X_test, **tokenizer_config).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485780c4-f0b9-43ab-9fbf-4feb0231b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] your ignorance is showing . the bat ##f warrant was un ##sea ##led . the entire operation was illegal from day one . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ...\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tokenizer.convert_ids_to_tokens(X_train_tokenized[\"input_ids\"][0][:50])), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd41f8-0071-4843-b841-8df82bc90114",
   "metadata": {},
   "source": [
    "**Exercise**: convert the dataset into a `tf.data.Dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1348c8b1-7b6f-47ab-9773-e158857266a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_tokenized, y_train))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val_tokenized, y_val))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_tokenized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ece08d-2075-4978-b17d-e060a50693ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetSpec(({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int64, name=None)), TensorShape([]))\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.DatasetSpec.from_value(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339bfeb-768f-46e3-ac8b-07affc4eb085",
   "metadata": {},
   "source": [
    "**Exercise**: shuffle, repeate and batch the training data, batch the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bca770f-bd94-45b7-8e6a-76a9f4bc9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(2 * CONFIG[\"batch_size\"])\n",
    "    .repeat()\n",
    "    .batch(CONFIG[\"batch_size\"])\n",
    ")\n",
    "batched_val_ds = (\n",
    "    val_ds\n",
    "    .batch(CONFIG[\"batch_size\"])\n",
    ")\n",
    "batched_test_ds = (\n",
    "    test_ds\n",
    "    .batch(CONFIG[\"batch_size\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e1b73-32d8-4a65-b7e2-0808b81057c4",
   "metadata": {},
   "source": [
    "## Importing the Pretrained Model\n",
    "\n",
    "**Exercise**: import the `TFDistilBertModel` from huggingface, with the model name as specified in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb328b59-8835-4bed-96e7-b72a7c2e4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 13:58:14.397492: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "\n",
    "transformer = TFDistilBertModel.from_pretrained(CONFIG[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8157817-ab76-401b-bda2-f8f5b6ef9b44",
   "metadata": {},
   "source": [
    "**Exercise**: define two input layers to feed the `input_id` and `attention_mask` sequences into the transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d2c53b0-0810-48a2-8c51-ad2b6679cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(CONFIG[\"seq_length\"],), name='input_ids', dtype='int32')\n",
    "attention_mask = tf.keras.layers.Input(shape=(CONFIG[\"seq_length\"],), name='attention_mask', dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd5626-c05a-4097-8f00-8039ef0c0b77",
   "metadata": {},
   "source": [
    "**Exercise**: extract the hidden representation of the `[CLS]` special token (always the first of every document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d53c33-a387-4672-a67f-807cd83b1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# We only care about DistilBERT's hidden-state of the last layer\n",
    "seq_embeddings = E1.last_hidden_state\n",
    "# We only care about DistilBERT's output for the [CLS] token, \n",
    "# which is located at index 0 of every encoded sequence \n",
    "# seq_embeddings.shape = (batch_size, seq_length, representation_length) -> we want the first of the sequence\n",
    "cls_embedding = seq_embeddings[:,0,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d3057-12ef-4473-ba51-fcad5fa3802b",
   "metadata": {},
   "source": [
    "## Defining a Classification Head\n",
    "\n",
    "**Exercise**: define a classification head with 5 layers: Dropout, Dense, Dropout, Dense, Output. The output layer needs to have the same number of dimensions as there are classes (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8f2410d-f2f7-4a27-8d30-ac42bf6241cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(0.2)(cls_embedding)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(CONFIG[\"num_classes\"], activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e868507-c50a-4d88-b17a-53b333e42ceb",
   "metadata": {},
   "source": [
    "**Exercise**: use the previously defined layer stack to define a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44771d61-2f13-491a-92b8-27f4dfab60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Model([input_ids, attention_mask], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281d951-cf58-407a-866e-15bea87623ec",
   "metadata": {},
   "source": [
    "**Exercise**: set the pretrained layer (`tf_distilbert_model`) to not trainable (we only want to train the head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b7f4974-d114-45c7-ab49-8ba900537f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_____________________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     Trainable  \n",
      "=============================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               Y          \n",
      "                                                                                                             \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               Y          \n",
      "                                                                                                             \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              N          \n",
      " BertModel)                     ast_hidden_state=(N               'attention_mask[0][0]']                    \n",
      "                                one, 512, 768),                                                              \n",
      "                                 hidden_states=None                                                          \n",
      "                                , attentions=None)                                                           \n",
      "                                                                                                             \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_distil_bert_model[0][0]']   Y          \n",
      " ingOpLambda)                                                                                                \n",
      "                                                                                                             \n",
      " dropout_19 (Dropout)           (None, 768)          0           ['tf.__operators__.getitem[0][0  Y          \n",
      "                                                                 ]']                                         \n",
      "                                                                                                             \n",
      " dense (Dense)                  (None, 128)          98432       ['dropout_19[0][0]']             Y          \n",
      "                                                                                                             \n",
      " dropout_20 (Dropout)           (None, 128)          0           ['dense[0][0]']                  Y          \n",
      "                                                                                                             \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout_20[0][0]']             Y          \n",
      "                                                                                                             \n",
      " dense_2 (Dense)                (None, 20)           1300        ['dense_1[0][0]']                Y          \n",
      "                                                                                                             \n",
      "=============================================================================================================\n",
      "Total params: 66,470,868\n",
      "Trainable params: 107,988\n",
      "Non-trainable params: 66,362,880\n",
      "_____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the pretrained part as non-trainable (we only want to train the head)\n",
    "#model.get_layer(\"input_ids\").trainable = False\n",
    "#model.get_layer(\"attention_mask\").trainable = False\n",
    "model.get_layer(\"tf_distil_bert_model\").trainable = False\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54734f35-71a6-4968-b14d-2a26c3b01280",
   "metadata": {},
   "source": [
    "**Exercise**: compile the model with `SparseCategoricalCrossentropy` as loss function and the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b54c8546-bb92-43a0-8a3c-8b641c75d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8258ee5-9043-4cf5-aafe-428fec31bb2a",
   "metadata": {},
   "source": [
    "**Exercise**: define three callbacks: `ModelCheckpoint`, `EarlyStopping`, and `TensorBoard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e28cbd0-9372-4e48-aff1-92852c7fd827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"finetuned_bert_20ng.hdf5\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3,          # Stop after 3 epochs of no improvement\n",
    "    monitor='val_loss',  # Look at validation_loss\n",
    "    min_delta=0.01,      # After 0 change\n",
    "    mode='min',          # Stop when quantity has stopped decreasing\n",
    "    verbose=1\n",
    ") \n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38301196-c584-42c2-9739-7bdbe7205ee9",
   "metadata": {},
   "source": [
    "**Exercise**: fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "928780e1-f821-4971-93bc-8c746846fb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    batched_train_ds, \n",
    "    validation_data = batched_val_ds,\n",
    "    epochs = 10, \n",
    "   # Define how many steps make up an epoch (bc. dataset is infinite)\n",
    "    steps_per_epoch = len(X_train) // CONFIG[\"batch_size\"],\n",
    "    callbacks = [model_checkpoint, early_stopping, tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a712e8-7dca-4e57-a62a-dfcebe2ee493",
   "metadata": {},
   "source": [
    "**Exercise**: load the best checkpoint from disk.\n",
    "\n",
    "*Notes*: \n",
    "- the model will likely be too big to train on you computer locally. You can get a finetuned version using the curl link below.\n",
    "- you need to specify the custom `TFDistilBertModel` pretrained class when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39014884-ed8c-4123-b264-16a6b01c5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  254M  100  254M    0     0  5596k      0  0:00:46  0:00:46 --:--:-- 7908k26  0:00:27 4761k:00:26 4843k46 --:--:-- 8004k\n"
     ]
    }
   ],
   "source": [
    "#!curl https://files.webis.de/bdlt-ss22/finetuned_bert_20ng.hdf5 --output finetuned_bert_20ng.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8466dc64-e025-4d75-953f-7adc0751ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.models.load_model(\n",
    "    \"finetuned_bert_20ng.hdf5\",\n",
    "    custom_objects={\n",
    "        'TFDistilBertModel': TFDistilBertModel, \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73849c7f-7e55-433a-b9d5-ed593517e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to python 401-Finetuned-Classification.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
