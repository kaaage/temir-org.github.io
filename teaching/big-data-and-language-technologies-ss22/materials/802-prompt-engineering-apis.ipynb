{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "## Using LLMs through APIs\n",
        "\n",
        "- *Course*: Big Data and Language Technologies\n",
        "- *Date*: 30.05.2022"
      ],
      "metadata": {
        "id": "hgp0pnswn50P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google/BIG-bench.git openai"
      ],
      "metadata": {
        "id": "Nr1uLcFxPqr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using GPT-3 (using the [API](https://beta.openai.com/docs/api-reference/completions/create) and its [Python bindings](https://github.com/openai/openai-python))"
      ],
      "metadata": {
        "id": "Wyk8ikG4dlgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "dzMpJHzuLRYn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter the API key from https://beta.openai.com/account/api-keys:"
      ],
      "metadata": {
        "id": "IOx19SgRT-YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"sk-...\""
      ],
      "metadata": {
        "id": "loZbiP2hMKd6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.Completion.create(engine=\"text-davinci-002\", prompt=\"Q: What is the answer to life, the universe and everything? A:\")"
      ],
      "metadata": {
        "id": "N-myjDlXSlfd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lring6PTXFl",
        "outputId": "0c29cd73-5cbf-4ea3-a3e0-115d21e33750"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Life, the universe and everything is 42.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"Q: How many meters in a mile? A:\",\n",
        "             \"Q: What is the color of the sky? A:\",\n",
        "             \"Q: Who you gonna call? A:\"]\n",
        "\n",
        "answers = [openai.Completion.create(engine=\"text-davinci-002\", prompt=q).choices[0].text for q in questions]\n",
        "\n",
        "for q,a in zip(questions,answers):\n",
        "  print(q,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aSVUUiqMbvi",
        "outputId": "4ddfa6aa-d988-4dd9-dcc0-daef2a04bda1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How many meters in a mile? A:  1609 meters\n",
            "Q: What is the color of the sky? A: \n",
            "\n",
            "The sky typically has a pale blue color.\n",
            "Q: Who you gonna call? A:  You should call one of our emergency locksmiths in London to deal with your\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using GPT-2 from Huggingface with the [model class from BIG-bench](https://github.com/google/BIG-bench/blob/main/bigbench/models/huggingface_models.py)"
      ],
      "metadata": {
        "id": "2nRczCn0eJVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bigbench.models.huggingface_models as huggingface_models\n",
        "gpt2_model = huggingface_models.BIGBenchHFModel(\"gpt2-large\")"
      ],
      "metadata": {
        "id": "vz6r--2AXUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt2_model.generate_text(\"Question: What is the answer to life, the universe and everything? Answer: It is\", max_length=64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1vHoR9iXmKL",
        "outputId": "1a21dfb5-98e2-4e99-ab65-8a88b3c09910"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " simple. And, it is not about 'discovering religion' anymore, bu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt2_model.generate_text(\"3 + 4 = \", max_length=16, output_regex=r\"[-+]?\\d+\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2O_7cZJY6TA",
        "outputId": "4ecd0a66-7089-4df6-ff18-f7e2248d08c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = gpt2_model.cond_log_prob(\n",
        "    inputs=\"What color is grass? Answer:\", \n",
        "    targets=('red', 'blue', 'green')\n",
        "    )\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1v4xM3SZLk6",
        "outputId": "f4768097-6a51-47c3-9f45-902b7f74da4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.5543476039559447, -1.6040321284920775, -0.3268762523324096]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using [OPT](https://huggingface.co/facebook/opt-350m) with the text generation pipeline from Huggingface"
      ],
      "metadata": {
        "id": "Fa6E05V7dckU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model=\"facebook/opt-350m\")"
      ],
      "metadata": {
        "id": "QFDJjsEYaXE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator(\"Q: What is the answer to life? A:\")[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTXOqEVAdJ1_",
        "outputId": "f7f160a3-b9b1-4b58-8f8a-9a30919881dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the answer to life? A: The answer is to be happy.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation using text-to-text metrics"
      ],
      "metadata": {
        "id": "u_sQ6vmef02r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from t5.evaluation import metrics"
      ],
      "metadata": {
        "id": "s8-_epzhlnxi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"This is a test.\"]\n",
        "candidates = [\"This is the test.\"]\n",
        "\n",
        "scores = metrics.bleu(references, candidates)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq_y-i_hnbcs",
        "outputId": "0697e173-6745-4a8e-c944-40459d07f095"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 30.213753973567677}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"This is a test.\"]\n",
        "candidates = [\"This is no test.\"]\n",
        "\n",
        "scores = metrics.bleu(references, candidates)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9mGUr7znkWi",
        "outputId": "5a08b87b-1543-44b6-9c54-771487abd4c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 30.213753973567677}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bleurt import score\n",
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n",
        "!unzip BLEURT-20.zip"
      ],
      "metadata": {
        "id": "r1LlYa1shfpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"BLEURT-20\"\n",
        "scorer = score.BleurtScorer(checkpoint)"
      ],
      "metadata": {
        "id": "OpeusK5WjaXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"This is a test.\"]\n",
        "candidates = [\"This is the test.\"]\n",
        "\n",
        "scores = scorer.score(references=references, candidates=candidates)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L55F1jPSkBjF",
        "outputId": "b76a9490-0596-4b2a-9402-ecf27a029fc1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7928394079208374]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"This is a test.\"]\n",
        "candidates = [\"This is no test.\"]\n",
        "\n",
        "scores = scorer.score(references=references, candidates=candidates)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Y-anHckVO4",
        "outputId": "7c0a1cc1-37a3-4d6d-9d34-f1c0babbde13"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6081441640853882]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LbdM44ybkZ4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}