{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a simple Retrieval Augmented Generation System\n",
        "\n",
        "## Step 0: Install dependencies\n",
        "We already do this for your. Here, we load\n",
        "- [ChatNoir](https://chatnoir.web.webis.de/) for retrieval and\n",
        "- the OpenAI Python library for generating responses."
      ],
      "metadata": {
        "id": "iyx2wbaQ3KWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE9FQPRc3GtD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/chatnoir-eu/chatnoir-api.git git+https://github.com/chatnoir-eu/chatnoir-pyterrier.git openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Add your API key here (please read carefully)\n",
        "\n",
        "By now, you should be onboarded to [Webis](https://webis.de) and should have gotten a login. Visit our Open WebUI instance at [open-webui.web.webis.de](https://open-webui.web.webis.de/) and login using your Webis GitLab account.\n",
        "\n",
        "Visit `Settings > Account > API keys > API Key`, add a new API key if it does not exist yet and copy your API key into the following string (it should start with `sk-`):"
      ],
      "metadata": {
        "id": "rQSxYWXDFIAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"sk-...\""
      ],
      "metadata": {
        "id": "HJRYqs1AEXqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Implement RAG\n",
        "\n",
        "Provided a query, Retrieval-Augmented Generation (RAG) consists of two steps:\n",
        "1. **Retrieving documents that are relevant to the query**<br/>This is exactly the same process as if you were to use Google or any other websearch engine to find relevant websites. Here, we use ChatNoir since it provides a simple API. You can learn more [here](https://github.com/chatnoir-eu/chatnoir-pyterrier). In our code this means that we need to implement the method\n",
        "```python\n",
        "def _retrieve(self, query: str) -> list[str]:\n",
        "```\n",
        "to query ChatNoir for the contents of relevant documents and return them as a list.\n",
        "2. **Generation**<br/>This is where the \"magic\" happens and what differentiates RAG from traditional websearch: Instead of dumping a list of links to the documents we found relevant, we give the query and the list of documents (called \"the context\") and ask a generative model to answer the query using the information provided in the context. To do so, use the [OpenAI Python API](https://github.com/openai/openai-python?tab=readme-ov-file#usage) to generate a response. For your convenience, we already created a method `_fetch_response(prompt: str) -> str` that does the generation for you but you need to come up with a way to combine the query and documents into a single prompt that lets the LLM generate a good response."
      ],
      "metadata": {
        "id": "3BdkQLHlGEA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "from chatnoir_api import Index\n",
        "from chatnoir_pyterrier import ChatNoirRetrieve, Feature\n",
        "from openai import OpenAI\n",
        "\n",
        "class RAG():\n",
        "\n",
        "  def __init__(self, topk=5) -> None:\n",
        "    self.llm = OpenAI(api_key=API_KEY, base_url=\"https://open-webui.web.webis.de/ollama/v1/\")\n",
        "    self.chatnoir = ChatNoirRetrieve(staging=True, num_results=topk, index=Index.MSMarcoV21, features=Feature.SNIPPET_TEXT)\n",
        "\n",
        "  def _fetch_response(self, prompt: str) -> str:\n",
        "    \"\"\"Prompts mistral:7b and returns the generated response.\"\"\"\n",
        "    completion = self.llm.chat.completions.create(\n",
        "        model=\"mistral:7b\",\n",
        "        messages=[\n",
        "            {\n",
        "              'role': 'user',\n",
        "              'content': prompt,\n",
        "            },\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_completion_tokens=200,\n",
        "        stream=True\n",
        "    )\n",
        "    return \"\".join(chunk.choices[0].delta.content for chunk in completion)\n",
        "\n",
        "  def _retrieve(self, query: str) -> list[str]:\n",
        "    \"\"\"Give a query, return a list of texts that may answer the query.\"\"\"\n",
        "    # TASK: Use ChatNoir to retrieve relevant documents and return only their\n",
        "    # content (\"snippet_text\").\n",
        "    # For more information on ChatNoir's API have a look at\n",
        "    #  https://github.com/chatnoir-eu/chatnoir-pyterrier\n",
        "    raise NotImplemented\n",
        "\n",
        "  def _generate(self, query: str, documents: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Takes a query and a list of documents that may answer the query and\n",
        "    generates a text answering the query using the information provided.\n",
        "    \"\"\"\n",
        "    # TASK: Come up with a prompt (a string) to generate an answer to the query\n",
        "    # using the context documents.\n",
        "    # You can generate text using self._fetch_response. To get started, you can\n",
        "    # have print out, what the model returns for example, if you write\n",
        "    # self._fetch_response(\"Who is Barack Obama\")\n",
        "    raise NotImplemented\n",
        "\n",
        "  def __call__(self, query: str) -> str:\n",
        "    return self._generate(query, self._retrieve(query))\n",
        "\n",
        "rag = RAG(topk=5)"
      ],
      "metadata": {
        "id": "1NePU8Hl3h10",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Success\n",
        "Now that the RAG system is implemented, you can test it by querying it for any information you like!"
      ],
      "metadata": {
        "id": "pmRqj5FBKBip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"How tall is the empire state building?\")"
      ],
      "metadata": {
        "id": "ghVBH6V8ELTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}