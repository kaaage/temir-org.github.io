{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this exercise, we will implement a logistic regression for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function\n",
    "\n",
    "Given a multiset of examples $D = \\{(\\mathbf{x}_1, c_1), ..., (\\mathbf{x}_n, c_n)\\}$ (the dataset), our learning task is to predict a class output under a logistic model function\n",
    "\n",
    "$$ y(x) = \\sigma(\\mathbf{w}^T\\mathbf{x}),$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}.$$\n",
    "\n",
    "The sigmoid function maps the space of real numbers to the $(0,1)$ interval, which allows us to interpret the output of the model function as probability for the event $\\mathbf{C} = 1$.\n",
    "\n",
    "**Exercise**: implement the sigmoid function and the model function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, X):\n",
    "    return sigmoid(X.dot(w.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "The loss function quantifies the error made by the model. Importantly, the loss has to be derivable, so we cannot simply use accuracy here. Instead, we rely on the pointwise logistic loss:\n",
    "\n",
    "$$\n",
    "l_{\\sigma}(c, y(\\mathbf{x})) = \\begin{cases} -\\log(y(\\mathbf{x})) & \\text{ if } c = 1 \\\\  -\\log(1 - y(\\mathbf{x})) & \\text{ if } c = 0 \\end{cases}.\n",
    "$$\n",
    "\n",
    "Please refer to the lecture slides for the derivation of this loss function.\n",
    "\n",
    "**Exercise**: implement the pointwise logistic loss for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, c):\n",
    "    \"\"\"\n",
    "    Pointwise logistic loss\n",
    "\n",
    "    :param y: prediction\n",
    "    :param c: ground-truth label\n",
    "    :return: pointwise logistic loss\n",
    "    \"\"\"\n",
    "    return c * -np.log(y) + (1 - c) * -np.log(1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "The last important part needed to assemble the training loop is the convergence function. As stated in the lecture slides, the function `convergence` analyses the norm of the loss gradient,\n",
    "\n",
    "$$||\\nabla L_\\sigma(\\mathbf{x}_t)|| = ||\\sum_{(\\mathbf{x},c)\\in D}(c-y_t(\\mathbf{x}))\\cdot\\mathbf{x}||,$$\n",
    "\n",
    "and compares it to a small positive bound $\\epsilon$. In addition, it may check if an upper bound for the number of iterations is reached. In comparison to the lecture, we simplify the implementation by calculating the pointwise loss gradient for each example individually, and then use the inner training loop to sum over all examples.\n",
    "\n",
    "This means that the convergence function for a single example will calculate $(c-y_t(\\mathbf{x}))\\cdot\\mathbf{x}$ only.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise**: implement the convergence function for a single example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence(x, c, y):\n",
    "    \"\"\"\n",
    "    Convergence function for a single example.\n",
    "\n",
    "    :param x: the feature vector for the example\n",
    "    :param c: the ground-truth class of the example\n",
    "    :param y: the model prediction for the example\n",
    "    :return: the pointwise direction of the steepest loss descent\n",
    "    \"\"\"\n",
    "    return (c - y) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "In order to fit our model, we rely on the logistic Batch Gradient Descent (BGD$_{\\sigma}$) algorithm.\n",
    "\n",
    "**Exercise**: implement the BGD$_{\\sigma}$ algorithm from the lecture. Some code is already filled in.\n",
    "\n",
    "*Remarks*:\n",
    "   - calculate the global logistic loss at every iteration and save the sum of loss values in an array; we want to visualize the loss after training, so it should be returned together with the optimal weights.\n",
    "   - calculate the convergence for each example using the function defined before and sum up their individual contributions; decide after the inner training loop whether convergence is reached or not. Remember to calculate the norm of the summed convergence values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(D, eta: float, eps: float, max_iter: int = 1000):\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent\n",
    "\n",
    "    :param D: multiset of examples (x, c), where x is a training vector and c is a class in {0, 1}.\n",
    "    :param eta: learning rate, a small positive constant\n",
    "    :param eps: epsilon value for convergence\n",
    "    :param max_iter: maximum number of training iterations (epochs)\n",
    "    :return: w (weight vector), l (loss vector)\n",
    "    \"\"\"\n",
    "    # Add bias dimension to data, inserting an extra 1 to every feature array\n",
    "    D = [(np.hstack([1, x]), c) for (x, c) in D]\n",
    "    # Initialize random w vector with random values + bias\n",
    "    w = (np.random.random(size=D[0][0].shape))\n",
    "    # Initialize an array to keep track of the loss\n",
    "    l = np.zeros(shape=max_iter, dtype=np.float64)\n",
    "\n",
    "    # Training loop\n",
    "    t = 0\n",
    "    while True:\n",
    "        # Reset w_delta\n",
    "        w_delta = 0\n",
    "        # Reset conv\n",
    "        conv = np.zeros_like(w)\n",
    "        # Batch loop\n",
    "        for (x, c) in D:\n",
    "            # Call model function to calculate prediction\n",
    "            y = model(w, x)\n",
    "            # Calculate the loss and increment l[t] by it\n",
    "            l[t] += loss(y, c)\n",
    "            # Calculate the convergence and increment conv by it\n",
    "            conv += convergence(x, c, y)\n",
    "            # Calculate delta\n",
    "            delta = c - y\n",
    "            # Add the step to w_delta\n",
    "            w_delta += eta * delta * x\n",
    "        # Update weights\n",
    "        w += w_delta\n",
    "        # Update timestep\n",
    "        t += 1\n",
    "        # Check if convergence is reached; remember to calculate the vector norm!\n",
    "        if np.linalg.norm(conv) <= eps or t >= max_iter:\n",
    "            break\n",
    "    # Return final weights and loss history (capped to current time step)\n",
    "    return w, l[:t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Using BGD, we can infer optimal weights for our model function. Yet, the output of the model function is the *probability* for each class, not the actual class label. Therefore, we need a `predict` function that takes a (batch of) training example(s), and returns the most probable class label. It is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{predict}(\\mathbf{x}) = \\begin{cases}1 & \\text{ if } y(\\mathbf{x}) >= 0.5 \\\\ 0 & \\text{ if } y(\\mathbf{x}) < 0.5\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "**Exercise**: implement a `predict` function.\n",
    "\n",
    "*Remarks*: remeber to also insert the bias value to every feature vector here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict class labels for given data.\n",
    "\n",
    "    :param X: data vector\n",
    "    :param w: weight vector\n",
    "    :return: predicted 0/1 class labels\n",
    "    \"\"\"\n",
    "    X = np.hstack([np.ones(shape=(X.shape[0], 1)), X])\n",
    "    return np.where(model(w, X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification Task\n",
    "\n",
    "To test the capabilities of the model, we conduct a sample classification task. The goal is to classify different wines (described by different features such as *Alcohol*, *Malic Acid*, *Magnesium*, etc.) into categories. Below, the dataset is loaded and the associated description with more info on its content is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature vectors X\n",
    "X = data.data\n",
    "# Load the class labels C\n",
    "C = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, wines are divided into three classes; to achieve a binary setting, we simply collapse label 2 and 3.\n",
    "\n",
    "**Exercise**: modify the class labels such that class 2 and 3 (labels 1 and 2) are both represented by label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.where(C == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the value ranges of features, we can observe that they are distributed extremely different: while some features are in the 0-1 range, feature 13 goes up to 1680."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.03</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.74</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.36</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.60</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.00</td>\n",
       "      <td>162.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.98</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.34</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.41</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.28</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.48</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.27</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>278.00</td>\n",
       "      <td>1680.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Minimum  Maximum\n",
       "0     11.03    14.83\n",
       "1      0.74     5.80\n",
       "2      1.36     3.23\n",
       "3     10.60    30.00\n",
       "4     70.00   162.00\n",
       "5      0.98     3.88\n",
       "6      0.34     5.08\n",
       "7      0.13     0.66\n",
       "8      0.41     3.58\n",
       "9      1.28    13.00\n",
       "10     0.48     1.71\n",
       "11     1.27     4.00\n",
       "12   278.00  1680.00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([X.min(axis=0), X.max(axis=0)]).T, columns=[\"Minimum\", \"Maximum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warrants *feature scaling*.  Intuitively, if we have incompatible ranges, features with larger ranges will dominate. Therefore, we apply *z-Standardization*, which means that every feature is rescaled to have a distribution of mean 0 and standard deviation of 1. We can use the existing implementation from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all features have compatible ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.434235</td>\n",
       "      <td>2.259772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.432983</td>\n",
       "      <td>3.109192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.679162</td>\n",
       "      <td>3.156325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.671018</td>\n",
       "      <td>3.154511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.088255</td>\n",
       "      <td>4.371372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.107246</td>\n",
       "      <td>2.539515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.695971</td>\n",
       "      <td>3.062832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.868234</td>\n",
       "      <td>2.402403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.069034</td>\n",
       "      <td>3.485073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.634288</td>\n",
       "      <td>3.435432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.094732</td>\n",
       "      <td>3.301694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.895054</td>\n",
       "      <td>1.960915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.493188</td>\n",
       "      <td>2.971473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Minimum   Maximum\n",
       "0  -2.434235  2.259772\n",
       "1  -1.432983  3.109192\n",
       "2  -3.679162  3.156325\n",
       "3  -2.671018  3.154511\n",
       "4  -2.088255  4.371372\n",
       "5  -2.107246  2.539515\n",
       "6  -1.695971  3.062832\n",
       "7  -1.868234  2.402403\n",
       "8  -2.069034  3.485073\n",
       "9  -1.634288  3.435432\n",
       "10 -2.094732  3.301694\n",
       "11 -1.895054  1.960915\n",
       "12 -1.493188  2.971473"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([X.min(axis=0), X.max(axis=0)]).T, columns=[\"Minimum\", \"Maximum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: assemble $D$ and fit the model using $BGD_{\\sigma}$.\n",
    "\n",
    "*Remarks*:\n",
    "- use 0.0001 as learning rate\n",
    "- use 40 as epsilon\n",
    "- use 1000 max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = list(zip(X, C))\n",
    "w, l = bgd(D, 1e-4, 40, max_iter=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: predict classes for every sample in $C$. Evaluate your result using either the `sklearn` evaluation metrics, or your own implementation from the last exercise. Play around with the model parameters and see if you can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88        59\n",
      "           1       0.96      0.91      0.94       119\n",
      "\n",
      "    accuracy                           0.92       178\n",
      "   macro avg       0.90      0.92      0.91       178\n",
      "weighted avg       0.92      0.92      0.92       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=C, y_pred=predict(X, w)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
