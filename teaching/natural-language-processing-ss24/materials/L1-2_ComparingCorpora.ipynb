{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e248e0",
   "metadata": {},
   "source": [
    "# Comparing Corpora\n",
    "\n",
    "The lecture talked about domain-specific languages or sub-languages. \n",
    "This manifests in various aspects of language like syntax, vocabulary or phrases.\n",
    "\n",
    "This exercise tries to confirm this by comparing subcorpora in terms of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9205",
   "metadata": {},
   "source": [
    "Download the brown corpus.\n",
    "\n",
    "_\"The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial\"_\n",
    "\n",
    "_--_ ( https://www.nltk.org/book/ch02.html ) \n",
    "\n",
    "\n",
    "To compare the vocabulary and phrases  of each corpus with each other, count ngram frequencies for every category. NLTK provides already pre-tokenized and cleaned text for this. Use `nltk.corpus.brown.words(category=...)` to load the corpus.\n",
    "\n",
    "The approach is to count frequencies in each corpus and compare the top k of each corpus with each other to spot differences. This allows a rather explorative view on the concept of sublanguage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb430356",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76688b37",
   "metadata": {},
   "source": [
    "Use for example the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20529109",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"news\", \"religion\", \"fiction\", \"humor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b8e67",
   "metadata": {},
   "source": [
    "You can use the word function to retrieve text for a category by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.corpus.brown.words(categories=\"news\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b125410",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Compare the corpora in terms of \n",
    "\n",
    "    - Type Token Ratio \n",
    "    - Vocabulary (most frequent words or ngrams)\n",
    "    - Most frequent syntactic structures (use nltk.pos_tag to generate POS-Tags and then look at the most frequent POS-tag ngrams) \n",
    "    \n",
    "\n",
    "Which problems arise if we only count frequencies and look at the most frequent ngrams? How can this be alleviated (hint: use the list `nltk.corpus.stopwords(\"en\")`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d03b1-3438-431d-bdbc-2c1de7412d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
