{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e248e0",
   "metadata": {},
   "source": [
    "# Comparing Corpora\n",
    "\n",
    "The lecture talked about domain-specific languages or sub-languages. \n",
    "This manifests in various aspects of language like syntax, vocabulary or phrases.\n",
    "\n",
    "This exercise tries to confirm this by comparing subcorpora in terms of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9205",
   "metadata": {},
   "source": [
    "Download the brown corpus.\n",
    "\n",
    "_\"The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial\"_\n",
    "\n",
    "_--_ ( https://www.nltk.org/book/ch02.html ) \n",
    "\n",
    "\n",
    "To compare the vocabulary and phrases  of each corpus with each other, count ngram frequencies for every category. NLTK provides already pre-tokenized and cleaned text for this. Use `nltk.corpus.brown.words(category=...)` to load the corpus.\n",
    "\n",
    "The approach is to count frequencies in each corpus and compare the top k of each corpus with each other to spot differences. This allows a rather explorative view on the concept of sublanguage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb430356",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76688b37",
   "metadata": {},
   "source": [
    "Use for example the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20529109",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"news\", \"religion\", \"fiction\", \"humor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b8e67",
   "metadata": {},
   "source": [
    "You can use the word function to retrieve text for a category by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.corpus.brown.words(categories=\"news\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b125410",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Compare the corpora in terms of \n",
    "\n",
    "    - Type Token Ratio \n",
    "    - Vocabulary (most frequent words or ngrams)\n",
    "    - Most frequent syntactic structures (use nltk.pos_tag to generate POS-Tags and then look at the most frequent POS-tag ngrams) \n",
    "    \n",
    "\n",
    "Which problems arise if we only count frequencies and look at the most frequent ngrams? How can this be alleviated (hint: use the list `nltk.corpus.stopwords(\"en\")`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d03b1-3438-431d-bdbc-2c1de7412d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {genre: list(nltk.corpus.brown.words(categories=genre)) for genre in genres}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f244cf-b717-4f7c-b4df-4c9a93f5b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, text in texts.items():\n",
    "    print(f\"{genre}: {text[:50]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ca40d-d0e4-4b30-bf4e-3e6d4e146f4a",
   "metadata": {},
   "source": [
    "#### Type Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968b47e-1417-4989-8df8-494fa8dca47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(text):\n",
    "    tokens = [x.lower() for x in text]\n",
    "    types = set(tokens)\n",
    "    return 100* len(types) / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b067772-6ef1-416a-b90c-ddc2d7a79780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ttrs(texts):\n",
    "    ttrs = [(k, ttr(v)) for k, v in texts.items()]\n",
    "    print(ttrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcc83a-1378-404b-94b8-acc1903af1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ttrs(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7751cf-f742-43a1-ba3d-bc6662772202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check corpora sizes\n",
    "for genre in texts:\n",
    "    print(f\"{genre}: {len(texts[genre])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe574a-f3fc-4eb7-9c37-3e921353b988",
   "metadata": {},
   "source": [
    "#### Standardised Type Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2f162-6677-42d1-8a33-1307af0129ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_chunks(text, chunk_size):\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        yield text[i:i+chunk_size]\n",
    "\n",
    "def sttr(text, chunk_size):\n",
    "    tokens = [x.lower() for x in text]\n",
    "    ttr = 0.0\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    for chunk in chunks:\n",
    "        types = set(chunk)\n",
    "        ttr += len(types)/len(chunk)\n",
    "    return ttr / len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a22c7-0b55-4337-a4ce-b87a0886b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sttr(texts, n):\n",
    "    sttrs = [(k, sttr(v, n)) for k,v in texts.items()]\n",
    "    return sttrs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede465f-d841-400b-b927-0c018527d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sttr(texts, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30505c1e-f907-4da1-b560-e5249af0a24b",
   "metadata": {},
   "source": [
    "#### N-Gram Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083542b3-315c-4922-9d58-7f805c7eb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, n):\n",
    "    text = [x.lower() for x in text]\n",
    "    ngrams = [x for x in nltk.ngrams(text, n)]\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\")) | set(string.punctuation) | {\"``\", '\"', \"''\", \"--\"}\n",
    "    ngrams = [x for x in ngrams if all(w not in stopwords for w in x)]\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e8d5d-0db9-45be-a76f-2b2805bc3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b7653-1cee-414d-914c-dbe14dc1dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_freqs(texts, n, topk=100):\n",
    "    word_freqs = { k:list(nltk.FreqDist(preprocess(v, n)))[:topk] for k, v in texts.items()}\n",
    "    return pd.DataFrame.from_dict(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83719b0-40d1-4fbb-a27b-3b91ca5f59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freqs(texts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba0d3b-6250-4ce1-8a83-8b2cd6ee3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freqs(texts, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc1730-d5ce-4d81-8a3f-ea3a91f5a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freqs(texts, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73ea51-98c2-427e-a5c5-48c29bc9e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freqs(texts, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f5a0e-edef-4d0f-9f0b-fb9545d7aaa8",
   "metadata": {},
   "source": [
    "#### POS Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386cf78-7311-41aa-abdf-95071ca054df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pos_pattern_frequencies(texts, n, topk=100):\n",
    "    pos_tags = {k:[x[1] for x in nltk.pos_tag(v)] for k, v in texts.items()}\n",
    "    pos_tags_ngrams = {k: nltk.ngrams(v, n) for k, v in pos_tags.items()}\n",
    "    frequencies = {k:nltk.FreqDist(v) for k,v in pos_tags_ngrams.items()}\n",
    "    compare = {k: list(v)[:topk] for k, v in frequencies.items()}\n",
    "    return pd.DataFrame.from_dict(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b6fd7-6369-407c-a691-25267a56fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pos_pattern_frequencies(texts, n=2, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911f91c-a48b-4e7b-b067-74533a2e9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pos_pattern_frequencies(texts, n=3, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af056d59-8e6d-4bdf-9a6d-077369210969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
