{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology\n",
    "\n",
    "Morphology is the study of the structure and formation of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Form Reduction\n",
    "\n",
    "In order to make meaningful statistics across inflected word forms in NLP, it is necessary to apply base form reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "- Sentence 1: Bernd liest ein Buch.\n",
    "- Sentence 2: Gestern las Bernd eine Zeitung.\n",
    "- Sentence 3: Lesen ist ein Hobby von Bernd.\n",
    "\n",
    "Sentence-Word-Matrix: \n",
    "\n",
    "|    | Bernd | liest | ein | Buch | Gestern | las | eine | Zeitung | Lesen | ist | Hobby | von |\n",
    "|----|-------|-------|-----|------|---------|-----|------|---------|-------|-----|-------|-----|\n",
    "| S1 | 1     | 1     | 1   | 1    | 0       | 0   | 0    | 0       | 0     | 0   | 0     | 0   |\n",
    "| S2 | 1     | 0     | 0   | 0    | 1       | 1   | 1    | 1       | 0     | 0   | 0     | 0   |\n",
    "| S3 | 1     | 0     | 1   | 0    | 0       | 0   | 0    | 0       | 1     | 1   | 1     | 1   |\n",
    "\n",
    "most frequent occurrences with \"Bernd\":\n",
    "\n",
    "| Word  | Occurrences with \"Bernd\" |\n",
    "|-------|--------------------------|\n",
    "| ein   | 2                        |\n",
    "| eine  | 1                        |\n",
    "| Hobby | 1                        |\n",
    "| liest | 1                        |\n",
    "| ...   | 1                        |\n",
    "\n",
    "\n",
    "--> **Base Form Reduction** \n",
    "\n",
    "- Sentence 1: bernd lesen einen buch.\n",
    "- Sentence 2: gestern lesen bernd einen zeitung.\n",
    "- Sentence 3: lesen sein einen hobby von bernd.\n",
    "\n",
    "\n",
    "|    | bernd | lesen | einen | buch | gestern | zeitung | sein | hobby | von |\n",
    "|----|-------|-------|------ |------|---------|---------|------|-------|-----|\n",
    "| S1 | 1     | 1     | 1     | 1    | 0       | 0       | 0    | 0     | 0   |\n",
    "| S2 | 1     | 1     | 1     | 0    | 1       | 1       | 0    | 0     | 0   |\n",
    "| S3 | 1     | 1     | 1     | 0    | 0       | 0       | 1    | 1     | 1   |\n",
    "\n",
    "\n",
    "most frequent occurrences with \"Bernd\":\n",
    "\n",
    "| Word   | Occurrences with \"Bernd\" |\n",
    "|------- |--------------------------|\n",
    "| lesen  | 3                        |\n",
    "| einen  | 3                        |\n",
    "| buch   | 1                        |\n",
    "| ...    | ....                     |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Why is it necessary to perform base form reduction?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "What errors can occur during base form reduction?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "What are the different approaches?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "What is the difference between stemming and lemmatization?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming with nltk\n",
    "\n",
    "NLTK also implements **lemmatization** and **stemming** techniques.\n",
    "\n",
    "**Stemming** returns the word stem of any word. It is context insensitive, tokens with different meaning can be projected to the same word stem.\n",
    "\n",
    "\n",
    "**Lemmatization** tries to return the lemma of any word. Grammatical context and function is important for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.stem.PorterStemmer().stem(\"wrote\"))\n",
    "print(nltk.stem.PorterStemmer().stem(\"writings\"))\n",
    "print(nltk.stem.PorterStemmer().stem(\"written\"))\n",
    "print(nltk.stem.PorterStemmer().stem(\"writes\"))\n",
    "print(nltk.stem.PorterStemmer().stem(\"write\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"wrote\", pos=\"v\"))\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"writings\", pos=\"v\"))\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"written\", pos=\"v\"))\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"writes\", pos = \"v\"))\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"write\", pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"writing\", pos=\"s\")) # if the word is a noun\n",
    "print(nltk.stem.WordNetLemmatizer().lemmatize(\"writing\", pos=\"v\")) # if the word is a verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Task\n",
    "\n",
    "We use Herman Meville's Moby Dick as a data base. We can download this from the Gutenberg Text Collection via the *nltk* package.\n",
    "\n",
    "- As a baseline for base form reduction, use the first 4 letters of each word. \n",
    "- Use Porter Stemmer as an example for stemming.\n",
    "- Use the **Spacy** library as an example for lemmatization.\n",
    "- Calculate the number of base forms to number of type ratio and compare the 3 approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#!conda install -y spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "#nltk.download()\n",
    "text = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "tokenized_text = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "sentences = nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # choose spacy model to use (needs to be downloaded first, see cell above)\n",
    "doc = nlp(\" \".join(sentences[80]))\n",
    "\n",
    "# spacy Doc example\n",
    "for t in doc:\n",
    "    print(t.text, t.lemma_, t.pos_, t.tag_, t.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 4-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseform_four_grams = [word[0:4] if len(word)>4 else word for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "baseform_porter_stemmer = [ps.stem(w) for w in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseform_spacy_lemmatization = [token.lemma_ for sen in sentences for token in nlp(\" \".join(sen))] # this might take a bit longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of the number of different basic forms to different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_types = len(set(tokenized_text))\n",
    "\n",
    "print(\"4 grams:\")\n",
    "print(len(set(baseform_four_grams))/number_of_types)\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "print(len(set(baseform_porter_stemmer))/number_of_types)\n",
    "\n",
    "print(\"spaCy Lemmatization:\")\n",
    "print(len(set(baseform_spacy_lemmatization))/number_of_types)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc66aa4dd7d71b5ab2c8fad87aef68b6dfe1409dab4ec716f2ee263efff84f87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
