{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0fe0d1",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "In this exercise we try to use the previously learned methods to develop a ngram based Language Detection module.\n",
    "\n",
    "Language Detection is the task of deciding the language of a given text using only the textual surface. \n",
    "\n",
    "The main assumption is that character statistics are very specific for any given language using the same alphabet.\n",
    "Using language specific corpora we can establish the ngram statistic of this given language by counting all the occuring ngrams.\n",
    "\n",
    "The assumption for the classification of new texts is that the most probable ngrams of a language are also more likely to appear in any new text.\n",
    "\n",
    "So the algorithm is:\n",
    " - Clean the corpus in a way that it only contains (ascii) letters of the alphabet (no punctuation and digits)\n",
    " - Given a corpus of some language, count all the character ngrams and generate a list of the top `k` ngrams. This list should contain unique entries and only character ngrams occurring within tokens (no spaces).\n",
    " - Given a new text generate a list of ngrams occurring within the words. (This list does not have to be unique, because the most likely ngrams of a language are also more likely to occurr.)\n",
    " - Count how many of these ngrams are among the top k ngrams of all the `known` languages.\n",
    " - The output of the algorithm is the language with the most absolute hits.\n",
    " \n",
    " \n",
    "As a formula:\n",
    "Let $ngrams^k_{l_i}$ the list of\n",
    "the top $k$ ngrams of language $l_i$\n",
    "\n",
    "Let $ngrams_{input}$ be the list of ngrams of a given input text $$ L(input) = \\text{argmax}_i (\\{g |g \\in l_i ,\\text{for g in } ngrams_{input}\\}) $$\n",
    " \n",
    " \n",
    "Try to distinguish between English and German. \n",
    "As English corpus use the complete gutenberg corpus from nltk.\n",
    "As German corpus use the tagesschau corpus we provided. (Download)\n",
    "\n",
    "**Hint**: Use the information from the power distribution over ngrams from the 'Zipf' notebook.\n",
    "\n",
    "\n",
    "__Deliberations__\n",
    " - What are the limits of this approach?\n",
    " - Where can you see problems?\n",
    " - You could also use Zipf's law for words to classify language by checking new sentences for words in the vocabulary. What would be the difference in approach, advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_text(path):\n",
    "    \"\"\"Load all the text files from the tagesschau folder\"\"\"\n",
    "    text = \"\"\n",
    "    for f in pathlib.Path(path).glob(\"*.txt\"):\n",
    "        with open(f, \"r\") as openf:\n",
    "            text += openf.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a30da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
