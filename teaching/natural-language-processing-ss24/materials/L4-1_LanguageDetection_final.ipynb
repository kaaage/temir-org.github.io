{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0fe0d1",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "In this exercise we try to use the previously learned methods to develop a ngram based Language Detection module.\n",
    "\n",
    "Language Detection is the task of deciding the language of a given text using only the textual surface. \n",
    "\n",
    "The main assumption is that character statistics are very specific for any given language using the same alphabet.\n",
    "Using language specific corpora we can establish the ngram statistic of this given language by counting all the occuring ngrams.\n",
    "\n",
    "The assumption for the classification of new texts is that the most probable ngrams of a language are also more likely to appear in any new text.\n",
    "\n",
    "So the algorithm is:\n",
    " - Clean the corpus in a way that it only contains (ascii) letters of the alphabet (no punctuation and digits)\n",
    " - Given a corpus of some language, count all the character ngrams and generate a list of the top `k` ngrams. This list should contain unique entries and only character ngrams occurring within tokens (no spaces).\n",
    " - Given a new text generate a list of ngrams occurring within the words. (This list does not have to be unique, because the most likely ngrams of a language are also more likely to occurr.)\n",
    " - Count how many of these ngrams are among the top k ngrams of all the `known` languages.\n",
    " - The output of the algorithm is the language with the most absolute hits.\n",
    " \n",
    " \n",
    "As a formula:\n",
    "Let $ngrams^k_{l_i}$ the list of\n",
    "the top $k$ ngrams of language $l_i$\n",
    "\n",
    "Let $ngrams_{input}$ be the list of ngrams of a given input text $$ L(input) = \\text{argmax}_i (\\{g |g \\in l_i ,\\text{for g in } ngrams_{input}\\}) $$\n",
    " \n",
    " \n",
    "Try to distinguish between English and German. \n",
    "As English corpus use the complete gutenberg corpus from nltk.\n",
    "As German corpus use the tagesschau corpus we provided. (Download)\n",
    "\n",
    "\n",
    "__Deliberations__\n",
    " - What are the limits of this approach?\n",
    " - Where can you see problems?\n",
    " - You could also use Zipf's law for words to classify language by checking new sentences for words in the vocabulary. What would be the difference in approach, advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312662d5",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db0c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_text(path):\n",
    "    text = \"\"\n",
    "    for f in pathlib.Path(path).glob(\"*.txt\"):\n",
    "        with open(f, \"r\") as openf:\n",
    "            text += openf.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70942956",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = load_german_text(\"./tagesschau_corpus/\")[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b65aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids())[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e38c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.\\n\\nSixteen years had Miss Taylor been in Mr. Woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of Emma.  Between _them_ it was more the intimacy\\nof sisters.  Even before Miss Taylor had ceased to hold the nominal\\noffice of governess, the mildness o\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4937f",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Pre-processing will consist by removing anything not alphabet, space or the german umlauts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c4390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def clean(s: str) -> str:\n",
    "    valid=string.ascii_letters+string.digits+\" äöüß\"\n",
    "    s = s.lower()\n",
    "    s = \"\".join([c if c in valid else \" \" for c in s ])\n",
    "    s = re.sub(\"[\\s]+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"before: {english_text[:100]}\")\n",
    "print(f\"after:  {clean(english_text[:100])}\")\n",
    "print(f\"before: {german_text[:100]}\")\n",
    "print(f\"after:  {clean(german_text[:100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deafac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = clean(english_text)\n",
    "german_text = clean(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53bc8382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endlich hat ein hilfskonvoi die schwer umkämpfte region um ost ghouta erreicht doch fehlen viele leb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4496f8",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2892b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nltk.ngrams([1, 2, 3, 4], 2)))\n",
    "print([\"-\".join(ngram) for ngram in nltk.ngrams(\"abcdef\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text:\", english_text[:10])\n",
    "print(\"first ngram tuple:\", next(nltk.ngrams(english_text, 3)))\n",
    "print(\"first 10 ngrams:\", [\"\".join(g) for g in nltk.ngrams(english_text, 3)][:10])\n",
    "print(\"frequencies for first 200 ngrams:\", nltk.FreqDist([\"\".join(g) for g in nltk.ngrams(english_text, 3)][:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410a8c5",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "883c67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageClassifier:\n",
    "    def __init__(self, texts, n=3, topk=1000):\n",
    "        \"\"\" Initialize the classifier \"\"\"\n",
    "        self.n = n\n",
    "        \n",
    "        # Count ngrams and only keep most common topk ngrams for each language\n",
    "        counts = {\n",
    "            k: nltk.FreqDist(self.create_ngrams(v)).most_common(topk) \n",
    "            for k, v in texts.items()\n",
    "        }\n",
    "        \n",
    "        # Remove the count, we are only interested in the list of ngrams\n",
    "        self.dicts = {\n",
    "            language: [x[0] for x in language_dicts]\n",
    "            for language, language_dicts in counts.items()\n",
    "        } \n",
    "        \n",
    "    def create_ngrams(self, texts):\n",
    "        \"\"\"Turn a string text into a list of character ngrams\"\"\"\n",
    "        return [\"\".join(g) for g in nltk.ngrams(texts, self.n)]\n",
    "    \n",
    "    def classify(self,text):\n",
    "        \"\"\"Classify an incoming text\"\"\"\n",
    "        \n",
    "        # Generate the ngrams for the input text\n",
    "        d =  self.create_ngrams(clean(text))\n",
    "        \n",
    "        # Allocate an empty dictionary to save the counts of ngram hits for the input text\n",
    "        counts = {} # Counts dictionary\n",
    "        \n",
    "        # Iterate over every language and its ngram lists\n",
    "        for language, language_dict in self.dicts.items():\n",
    "            counts[language] = sum([tr in language_dict for tr in d])   \n",
    "\n",
    "        # Sort list by frequency\n",
    "        sorted_counts = sorted(list(counts.items()), key=lambda x: -x[1])\n",
    "        print(sorted_counts)\n",
    "        # Predict the language with most hits\n",
    "        return sorted_counts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84582b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGLC = NGramLanguageClassifier({\"english\":english_text, \"german\":german_text}, topk=1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0145da5",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f29cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('german', 29), ('english', 21)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGLC.classify(\"Ich arbeite an der Universität Leipzig.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb4f0b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('english', 23), ('german', 20)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGLC.classify(\"I am working at the University.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e405c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('english', 6), ('german', 6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGLC.classify(\"Ich heiße John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e516853f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('german', 2), ('english', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGLC.classify(\"dies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e85bb2",
   "metadata": {},
   "source": [
    "### Deliberations\n",
    "\n",
    "Limits: \n",
    " - Memory problems when all N-grams are stored. This may not be a practical problem, considering the Zipf results.\n",
    " - No weighting of the features\n",
    " - Dependence on the training corpus\n",
    " - possibility to represent non-valid word forms in the language by N-gram combinations\n",
    "\n",
    "Problems:\n",
    "\n",
    "- wrong decisions as shown in the examples\n",
    "\n",
    "Using Words instead of N-grams:\n",
    "- dictionary of word forms (types)\n",
    "- more or less stop word check\n",
    "- always valid word forms\n",
    "- Problem: often out of vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
