{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c672af5",
   "metadata": {},
   "source": [
    "# TF-IDF Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5dae3",
   "metadata": {},
   "source": [
    "You have the following (tokenized) document collection:\n",
    "\n",
    "|id | words|\n",
    "|---|------|\n",
    "| 1 | like, like, fruit, fly, fly|\n",
    "| 2 | bee, wasp, like|\n",
    "| 3 | fruit, fly|\n",
    "| 4 | bee, wasp, fruit|\n",
    "| 5| fruit, fruit, fruit, fly|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2bfc7",
   "metadata": {},
   "source": [
    "1. Determine the Document Term Matrix.\n",
    "\n",
    "2. Calculate all the tf-idf values.\n",
    "\n",
    "Use max-normalization, such that the most frequent term of every document has relative term frequency 1.\n",
    "(Equivalent to k-normalization from the lecture with $K=0$). \n",
    "\n",
    "Use the natural logarithm for calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f256f",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Document Term Matrix:\n",
    "\n",
    "| term |  D1| D2|D3|D4|D5| IDF|\n",
    "|------|----|---|--|--|--|---|\n",
    "| bee| 0|1|0|1|0|\n",
    "| wasp|0|1|0|1|0| \n",
    "| like|2|1|0|0|0| \n",
    "| fruit|1|0|1|1|3|\n",
    "| fly|2|0|1|0|1|\n",
    "\n",
    "\n",
    "#### Tf, idf Values:\n",
    "\n",
    "| term |  D1| D2|D3|D4|D5| IDF|\n",
    "|------|----|---|--|--|--|---|\n",
    "| bee| 0|1|0|1|0|0.916|\n",
    "| wasp|0|1|0|1|0|0.916|\n",
    "| like|1|1|0|0|0| 0.916|\n",
    "| fruit|0.5|0|1|1|1|0.223|\n",
    "| fly|1|0|1|0|0.333|0.511|\n",
    "\n",
    "$N = 5$\n",
    "\n",
    "$ {idf}_{bee} = log(\\frac{5}{2}) = 0.916 $\n",
    "\n",
    "$ {tf}_{fruit @ D1} = \\frac{1}{2}$\n",
    "\n",
    "$ {tf}_{fly @ D5} = \\frac{1}{3}$ (D5: 3x _fruit_, 1x _fly_)\n",
    "\n",
    "\n",
    "#### Tf-idf Values:\n",
    "\n",
    "| term |  D1| D2|D3|D4|D5|\n",
    "|------|----|---|--|--|--|\n",
    "| bee  | 0  |0.916|0|0.916|0|\n",
    "| wasp |0   |0.916|0|0.916|0|\n",
    "| like |0.916   |0.916 |0|0|0|\n",
    "| fruit|0.112   |0|0.223|0.223|0.223|\n",
    "| fly  |0.511|0|0.511|0|0.170|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520816d",
   "metadata": {},
   "source": [
    "# A2 - Application Retrieval\n",
    "\n",
    "We can use the tf-idf table to determine the similarity of two documents. Each document can be represented as the vector of its tf-idf weighted words. The cosine of the two vectors is often used to calculate a similarity measure.\n",
    "\n",
    "When for example a user searches for \"fruit fly\", you can interpret the query as a new document and translate the query into a vector corresponding to your vocabulary. Calculate the cosine similarity with every document in your corpus and  return the best match.\n",
    "\n",
    "$$ cos(\\vec{q},\\vec{d}) = \\frac{\\vec{q}\\cdot\\vec{d}}{\\lVert\\vec{q}\\rVert\\cdot\\lVert\\vec{d}\\rVert}$$\n",
    "\n",
    "\n",
    "Which document would the queries $Q_1=[fruit, fly]$ and $Q_2=[bee, fly]$ return?\n",
    "\n",
    "To generate a query vector, create a vector that has the same dimension as the vocabulary and 1 if a word occurs in the query and zero everywhere else. (One-Hot Encoding)\n",
    "\n",
    "What are limitations and problems \n",
    "of this approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770b91b",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "$\\vec{Q} = (0,0,0,1,1)^T$\n",
    "\n",
    "Similarities :\n",
    "\n",
    "| D1| D2| D3 | D4 | D5|\n",
    "|---|---|----|----|---|\n",
    "| 0.417| 0. |0.931| 0.159| **0.991**|\n",
    "\n",
    "Document D5 would be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ebfe3",
   "metadata": {},
   "source": [
    "\n",
    "$\\vec{Q} = (1,0,0,0,1)^T$\n",
    "\n",
    "Similarities :\n",
    "\n",
    "| D1| D2| D3 | D4 | D5|\n",
    "|---|---|----|----|---|\n",
    "|0.343| 0.408|**0.648**| 0.487| 0.429|\n",
    "\n",
    "Document D3 would be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226980ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities for Q1: [0.41761867 0.         0.93090556 0.11996042 0.99102857]\n",
      "Similarities for Q2: [0.34254116 0.40824829 0.64808276 0.49275222 0.4286892 ]\n"
     ]
    }
   ],
   "source": [
    "# Example Code numpy:\n",
    "import numpy as np\n",
    "q1 = np.array([0,0,0,1,1])\n",
    "q2 = np.array([1,0,0,0,1])\n",
    "d = np.array([[0,0,0.916,0.112,0.511], # D1\n",
    "              [0.916,0.916,0.916,0,0], # D2\n",
    "              [0,0,0,0.223,0.511],\n",
    "              [0.916,0.916,0,0.223,0],\n",
    "              [0,0,0,0.223,0.170]]) # D5\n",
    "\n",
    "# @-operator = matmul (Matrix Multiplication)\n",
    "print(f\"Similarities for Q1: {d@q1 / (np.linalg.norm(q1)*np.linalg.norm(d, axis=1))}\")\n",
    "print(f\"Similarities for Q2: {d@q2 / (np.linalg.norm(q1)*np.linalg.norm(d, axis=1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b0651",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "- OOV, words can occurr in the query but not the corpus (Out-of-vocabulary)\n",
    "- Memory when scaling to the vocabulary of large corpora (technical problem)\n",
    "- No disambiguation\n",
    "- Stemming, wordforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b2a29",
   "metadata": {},
   "source": [
    "### Computation Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6f8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.623\n",
      "[0.623 0.    0.734 0.223 0.393]\n",
      "1.4142135623730951\n",
      "[1.05485591 1.58655854 0.55753924 1.31447366 0.28040863]\n",
      "[1.49179154 2.2437326  0.78847955 1.85894648 0.39655769]\n"
     ]
    }
   ],
   "source": [
    "#q1   = [0, 0,     0,     1,     1]\n",
    "#d[0] = [0, 0, 0.916, 0.112, 0.511] # D1\n",
    "r = sum([ai * bi for ai,bi in zip(q1, d[0])])  # = d[0]@q1\n",
    "print(r)\n",
    "\n",
    "print(d@q1) # = np.dot(d, q1); all docs in d with q1\n",
    "print(np.linalg.norm(q1)) # norm of query\n",
    "print(np.linalg.norm(d, axis=1)) # norm of docs\n",
    "print(np.linalg.norm(q1)*np.linalg.norm(d, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
