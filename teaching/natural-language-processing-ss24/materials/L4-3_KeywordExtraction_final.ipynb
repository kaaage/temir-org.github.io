{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c191342c",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "The simple count based method to extract sublanguage specific vocabulary only allows explorative approaches. It gives no objective measurement of how specific a word is to a sublanguage corpus.\n",
    "To alleviate this problem we can either use Log-Likelihood or tf-idf to extract sublanguage specific vocabulary.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "To determine the difference between 2 or more sources, we have to formulate a weight for the\n",
    "each word with regards to each text source. One possible measure is the tf-idf measure which is a weighting based on the unique usage of a term in single documents. The more often a term is used in different\n",
    "documents the less importance it gets w.r.t. the tf-idf weight. In detail, this follows the intuition\n",
    "that a term which appears very often can’t be unique to a certain class or domain.\n",
    "Following Wikipedia, the tf-idf value increases proportionally to the number of times a word\n",
    "appears in the document, but is often offset by the frequency of the word in the corpus, which\n",
    "helps to adjust for the fact that some words appear more frequently in general.[1]\n",
    "\n",
    "For normalized term frequency $tf(t,D)$ there are various options (see lecture, videos in moodle or research).\n",
    "\n",
    "### Log Likelihood \n",
    "Another possibility to measure relative importance of words is Log-Likelihood.\n",
    "When using a reference corpus for comparison we use the word-counts in the different domains and\n",
    "a reference corpus in order to determine significant differences. \n",
    "The used significance test is called the “Log-Likelihood”-Ratio Test (LL). The LL-value gives the expectation of a term to be appearing in the target w.r.t. the reference\n",
    "corpus. \n",
    "\n",
    "\n",
    "### Corpora\n",
    "\n",
    "We provide text for the three domains `Automobil`, `Wirtschaft` and `Sport`.\n",
    "When in need of a reference corpus, visit the [wortschatz-portal](https://wortschatz.uni-leipzig.de/de/download/German#deu_news_2021) and download a large enough sample of references, around 4 million sentences should suffice.\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "Be aware that the prepocessing of text has considerable influence on the outcome. Part of this exercise is to\n",
    "to deploy a reasonable preprocessing pipeline. Make use of the knowledge about the Zipf distribution and other text preprocessing techniques.\n",
    "\n",
    "To analyze differences we need to build a single \"document\" for each domain. This\n",
    "means, if there is more than one document per domain, we’ll concat all texts belonging to one domain to a single text source.\n",
    "\n",
    "### Also\n",
    "\n",
    "It makes sense to first introduce a function that transforms a collection of documents into an Document-Term-Matrix (DTM). For that, the numpy library's array class is worth a look. (In practice data sizes may quickly exceed memory. It is then necessary to consider data structures to accomodate for that, e.g. sparse arrays. In this exercise standard numpy arrays should suffice.)\n",
    "\n",
    "**Hint 1** If you use numpy, be aware that numpy contains a lot of useful functions like logarithms or sorting.\n",
    "\n",
    "**Hint 2** Beware of numerical traps like the undefined logarithm of 0.\n",
    "\n",
    "### Task\n",
    "\n",
    "Apply the two measures tf-idf and Log-Likelihood to extract the top keywords for the 3 corpora `Automobil`, `Wirtschaft` and `Sport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3438204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd # Imported for prettier output of keyword lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2126ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keyword/automobil_50k.txt': 'automobil', 'keyword/wirtschaft_50k.txt': 'wirtschaft', 'keyword/sport_50k.txt': 'sport'}\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "content = {}\n",
    "with zipfile.ZipFile(\"keyword.zip\") as zfile:\n",
    "    for f in zfile.namelist():\n",
    "        if f != \"keyword/\":\n",
    "            content[f] = zfile.read(f).decode(\"utf8\")\n",
    "\n",
    "topics = content.keys()\n",
    "topics = {t: t.split(\"/\")[-1].split(\"_\")[0] for t in topics}\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196913c1",
   "metadata": {},
   "source": [
    "### Single File Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df3aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_single_file = \"deu_news_2021_100K-sentences.txt\"\n",
    "with open(path_single_file) as fs:\n",
    "    reference = fs.readlines()\n",
    "    reference = \" \".join([line.split(\"\\t\")[1].lower() for line in reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5583b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "…(15) aus roitham, nachdem er mit seinem moped in einen traktor gekracht war.\n",
      " \"15 richter haben ja gesagt, aus unterschiedlichen politischen richtungen.\n",
      " \"1959 war es noch eine gesamtdeutsche mannsch\n"
     ]
    }
   ],
   "source": [
    "print(reference[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2c943",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    #txt = txt if len(txt[0]) > 1 else nltk.word_tokenize(txt)\n",
    "    txt = txt if len(txt[0]) > 1 else txt.split(\" \")\n",
    "    txt = [\n",
    "        x.replace(\"ß\",\"ss\").lower()\n",
    "        for x in txt\n",
    "        if x.isalpha() and len(x) > 1\n",
    "    ]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958ae8b",
   "metadata": {},
   "source": [
    "#### Create DTM in Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f1f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 111920\n",
      "vocab size: 34185\n"
     ]
    }
   ],
   "source": [
    "# def create_dtm()\n",
    "\n",
    "# pruning of vocabulary\n",
    "cut_first=200 # prune top 200 (stopwords etc.)\n",
    "min_freq=3 # at least 3 occurrences required else probably not important enough for all texts\n",
    "\n",
    "# Clean texts\n",
    "texts = {k: preprocess(v) for k,v in content.items()}\n",
    "        \n",
    "# Count texts' global vocabulary\n",
    "vocab = nltk.FreqDist(sum(texts.values(),[]))\n",
    "print(\"vocab size:\", len(vocab))\n",
    "\n",
    "# Prune overall vocabulary\n",
    "vocab = sorted(list(vocab.items()), key=lambda x: -x[1]) \n",
    "vocab = vocab[cut_first:]\n",
    "vocab = [x[0] for x in vocab if x[1] >= min_freq]\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629b3d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM   shape (3, 34185)\n",
      "[[440 817 333 ...   0   0   0]\n",
      " [214  32 222 ...   0   0   0]\n",
      " [206  11 301 ...   3   3   3]]\n",
      "\n",
      "words shape (34185,)\n",
      "['kaum' 'wagen' 'lange' ... 'nielsen' 'tamara' 'huh']\n",
      "\n",
      "topics shape (3, 1)\n",
      "[['automobil']\n",
      " ['wirtschaft']\n",
      " ['sport']]\n",
      "\n",
      "tf    shape (3, 34185)\n",
      "[[0.53855569 1.         0.40758874 ... 0.         0.         0.        ]\n",
      " [0.38282648 0.05724508 0.39713775 ... 0.         0.         0.        ]\n",
      " [0.27357238 0.01460823 0.3997344  ... 0.00398406 0.00398406 0.00398406]]\n",
      "\n",
      "idf   shape (34185,)\n",
      "[0.         0.         0.         ... 1.09861229 1.09861229 1.09861229]\n",
      "\n",
      "tfidf shape (3, 34185)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.00437694 0.00437694 0.00437694]]\n"
     ]
    }
   ],
   "source": [
    "# count and restrict domain level text to the vocabulary\n",
    "# 1. all text specific keywords\n",
    "term_frequencies = {genre: nltk.FreqDist(text) for genre, text in texts.items()}\n",
    "# 2. Create the global DTM by iterating over the global (!) voabulary.\n",
    "dtm = np.array([[v.get(w, 0) for w in vocab] for k,v in term_frequencies.items()])\n",
    "print(\"DTM   shape\", dtm.shape)\n",
    "print(dtm)\n",
    "\n",
    "# columns\n",
    "words = np.array([w for w in vocab])\n",
    "print(\"\\nwords shape\", words.shape)\n",
    "print(words)\n",
    "\n",
    "# rows\n",
    "topics = content.keys()\n",
    "topics = np.array([t.split(\"/\")[-1].split(\"_\")[0] for t in topics])[:,np.newaxis]\n",
    "print(\"\\ntopics shape\", topics.shape)\n",
    "print(topics)\n",
    "\n",
    "\n",
    "# 3. compute term frequencies / inverse document frequencies\n",
    "tf = dtm / dtm.max(-1, keepdims=True)\n",
    "idf = np.log(dtm.shape[0] / ((dtm>0).sum(0) + 1e-25))\n",
    "print(\"\\ntf    shape\", tf.shape)\n",
    "print(tf)\n",
    "print(\"\\nidf   shape\", idf.shape)\n",
    "print(idf)\n",
    "# 4. TF-IDF\n",
    "tfidf = tf * idf\n",
    "print(\"\\ntfidf shape\", tfidf.shape)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc94a65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: keyword/automobil_50k.txt\n",
      "['hubraum', 'litern', 'diesel', 'autofahrer', 'verbrauch', 'roadster', 'coupé', 'fahrzeugs', 'durchschnittsverbrauch', 'design']\n"
     ]
    }
   ],
   "source": [
    "# extract keywords (sort by decreasing tf-idf, take top-n words)\n",
    "docnr = 0\n",
    "n = 10\n",
    "print(\"doc:\", list(content.keys())[docnr])\n",
    "words = [vocab[k] for k in tfidf[docnr].argsort(0)[::-1][:n]]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d422e5",
   "metadata": {},
   "source": [
    "### Keyword Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c85a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def create_dtm(self, texts, cut_first=200, min_freq=3):\n",
    "        \n",
    "        # Clean texts\n",
    "        self.texts = {k: preprocess(v) for k,v in texts.items()}\n",
    "        \n",
    "        # Count texts' global vocabulary\n",
    "        self.vocab = nltk.FreqDist(sum(self.texts.values(),[]))\n",
    "        \n",
    "        # Prune overall vocabulary\n",
    "        self.vocab = sorted(list(self.vocab.items()), key=lambda x: -x[1]) \n",
    "        self.vocab = [x[0] for x in self.vocab[cut_first:] if x[1] >= min_freq]\n",
    "        \n",
    "        # count and restrict domain level text to the vocabulary\n",
    "        # 1. all text specific keywords\n",
    "        self.term_frequencies = {genre: nltk.FreqDist(text) for genre, text in self.texts.items()}\n",
    "        # 2. Create the global DTM by iterating over the global (!) voabulary.\n",
    "        self.dtm = np.array([[v.get(w,0) for w in self.vocab] for k,v in self.term_frequencies.items()])\n",
    "    \n",
    "    def tfidf(self):\n",
    "        #tf = np.log(self.dtm + 1e-25)# alternative normalization: Logarithmic\n",
    "        tf = self.dtm / self.dtm.max(-1, keepdims=True)\n",
    "        idf = np.log(self.dtm.shape[0] / ((dtm>0).sum(0) + 1e-25))\n",
    "        return tf * idf\n",
    "\n",
    "    def tfidf_keywords(self, n=10):\n",
    "        \"\"\"Iterate all copora for printing\"\"\"\n",
    "        tfidf = self.tfidf()\n",
    "        return {\n",
    "            k: [\n",
    "                self.vocab[k] for k in tfidf[i].argsort(0)[::-1][:n]\n",
    "            ]\n",
    "            for i,k in enumerate(self.texts.keys())\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7838982",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = TFIDF()\n",
    "da.create_dtm(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296fe2ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword/automobil_50k.txt</th>\n",
       "      <th>keyword/wirtschaft_50k.txt</th>\n",
       "      <th>keyword/sport_50k.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hubraum</td>\n",
       "      <td>dax</td>\n",
       "      <td>tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>litern</td>\n",
       "      <td>gdl</td>\n",
       "      <td>vfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diesel</td>\n",
       "      <td>verlinken</td>\n",
       "      <td>sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>autofahrer</td>\n",
       "      <td>zentralbank</td>\n",
       "      <td>hertha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verbrauch</td>\n",
       "      <td>aktien</td>\n",
       "      <td>borussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>roadster</td>\n",
       "      <td>commerzbank</td>\n",
       "      <td>kader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coupé</td>\n",
       "      <td>index</td>\n",
       "      <td>torhüter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fahrzeugs</td>\n",
       "      <td>mehdorn</td>\n",
       "      <td>verlinken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>durchschnittsverbrauch</td>\n",
       "      <td>ubs</td>\n",
       "      <td>tabellenführer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>design</td>\n",
       "      <td>arbeitsmarkt</td>\n",
       "      <td>fsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>esp</td>\n",
       "      <td>karstadt</td>\n",
       "      <td>tsg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>serienmässig</td>\n",
       "      <td>anleger</td>\n",
       "      <td>partien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>gm</td>\n",
       "      <td>bsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rückbank</td>\n",
       "      <td>aktienmarkt</td>\n",
       "      <td>dfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mini</td>\n",
       "      <td>währungsfonds</td>\n",
       "      <td>durchgang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>klimaanlage</td>\n",
       "      <td>finanzmärkten</td>\n",
       "      <td>gaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>benziner</td>\n",
       "      <td>arcandor</td>\n",
       "      <td>vfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mittelkonsole</td>\n",
       "      <td>iwf</td>\n",
       "      <td>freistoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mazda</td>\n",
       "      <td>lehman</td>\n",
       "      <td>hoffenheim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lamborghini</td>\n",
       "      <td>tecdax</td>\n",
       "      <td>finale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>heck</td>\n",
       "      <td>notenbank</td>\n",
       "      <td>tore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motorhaube</td>\n",
       "      <td>siemens</td>\n",
       "      <td>sc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>drehmoment</td>\n",
       "      <td>rezession</td>\n",
       "      <td>torwart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kombi</td>\n",
       "      <td>mdax</td>\n",
       "      <td>rekordmeister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>selbstzünder</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>nowitzki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword/automobil_50k.txt keyword/wirtschaft_50k.txt keyword/sport_50k.txt\n",
       "0                    hubraum                        dax                   tsv\n",
       "1                     litern                        gdl                   vfb\n",
       "2                     diesel                  verlinken                    sg\n",
       "3                 autofahrer                zentralbank                hertha\n",
       "4                  verbrauch                     aktien              borussia\n",
       "5                   roadster                commerzbank                 kader\n",
       "6                      coupé                      index              torhüter\n",
       "7                  fahrzeugs                    mehdorn             verlinken\n",
       "8     durchschnittsverbrauch                        ubs        tabellenführer\n",
       "9                     design               arbeitsmarkt                   fsv\n",
       "10                       esp                   karstadt                   tsg\n",
       "11              serienmässig                    anleger               partien\n",
       "12                kleinwagen                         gm                   bsc\n",
       "13                  rückbank                aktienmarkt                   dfb\n",
       "14                      mini              währungsfonds             durchgang\n",
       "15               klimaanlage              finanzmärkten                  gaal\n",
       "16                  benziner                   arcandor                   vfl\n",
       "17             mittelkonsole                        iwf             freistoss\n",
       "18                     mazda                     lehman            hoffenheim\n",
       "19               lamborghini                     tecdax                finale\n",
       "20                      heck                  notenbank                  tore\n",
       "21                motorhaube                    siemens                    sc\n",
       "22                drehmoment                  rezession               torwart\n",
       "23                     kombi                       mdax         rekordmeister\n",
       "24              selbstzünder                  microsoft              nowitzki"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(da.tfidf_keywords(n=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1716a",
   "metadata": {},
   "source": [
    "--> https://temir.org/teaching/natural-language-processing-ss24/materials/part-en-nlp-keywords.pdf  \n",
    "--> `NLP:IX-20 NLP Applications`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08e269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLike:\n",
    "    def create_dtm(self, texts, cut_first=200, min_freq=3):\n",
    "        \n",
    "        # Clean texts\n",
    "        self.texts = {k:preprocess(v) for k,v in texts.items()}\n",
    "        \n",
    "        # Count texts' global vocabulary\n",
    "        self.vocab = nltk.FreqDist(sum(self.texts.values(),[]))\n",
    "        \n",
    "        # Prune overall vocabulary\n",
    "        self.vocab = sorted(list(self.vocab.items()),key= lambda x: -x[1]) \n",
    "        self.vocab = [x[0] for x in self.vocab[cut_first:] if x[1] >=min_freq]\n",
    "        \n",
    "        # count and restrict domain level text to the vocabulary\n",
    "        # 1. all text specific keywords\n",
    "        self.term_frequencies = {genre: nltk.FreqDist(text) for genre, text in self.texts.items()}\n",
    "        # 2. Create the global DTM by iterating over the global (!) voabulary.\n",
    "        self.dtm = np.array([[v.get(w,0) for w in self.vocab] for k,v in self.term_frequencies.items()])\n",
    "\n",
    "    def log_likelihood(self, corpus, n=None, threshold=None):\n",
    "        i = list(da.texts.keys()).index(corpus)\n",
    "       \n",
    "        # document for corpus (a) and reference (b)\n",
    "        a = self.dtm[i]\n",
    "        b = self.dtm[list(da.texts.keys()).index(\"reference\")]\n",
    "\n",
    "        # total frequency (count tokens)\n",
    "        c = a.sum()\n",
    "        d = b.sum()\n",
    "\n",
    "        # log-likelihood test\n",
    "        e1 = c * (a + b) / (c + d) + 1e-25\n",
    "        e2 = d * (a + b) / (c + d) + 1e-25\n",
    "\n",
    "        ll = 2 * (a * np.log(a / e1 + 1e-25) + b * np.log(b / e2 + 1e-25))\n",
    "        # likelihood estimate for each word in vocabulary\n",
    "\n",
    "        if threshold is not None:\n",
    "            return [da.vocab[k.item()] for k in np.where(ll > threshold)[0]]\n",
    "\n",
    "        if n is not None:\n",
    "            return [self.vocab[k] for k in ll.argsort(0)[::-1][:n]]\n",
    "\n",
    "\n",
    "    def ll_keywords(self, n=None, threshold=None):\n",
    "        return {k: self.log_likelihood(k, n=n, threshold=threshold) for k in self.texts.keys() if k != \"reference\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eff41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[\"reference\"] = reference # Add the reference corpus\n",
    "da = LogLike()\n",
    "da.create_dtm(content, cut_first=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a0489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword/automobil_50k.txt</th>\n",
       "      <th>keyword/wirtschaft_50k.txt</th>\n",
       "      <th>keyword/sport_50k.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ps</td>\n",
       "      <td>bank</td>\n",
       "      <td>trainer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liter</td>\n",
       "      <td>dollar</td>\n",
       "      <td>mannschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>banken</td>\n",
       "      <td>saison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>polizei</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wagen</td>\n",
       "      <td>opel</td>\n",
       "      <td>verlinken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vw</td>\n",
       "      <td>verlinken</td>\n",
       "      <td>sieg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mercedes</td>\n",
       "      <td>konzern</td>\n",
       "      <td>kostenfrei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>motor</td>\n",
       "      <td>kostenfrei</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>opel</td>\n",
       "      <td>link</td>\n",
       "      <td>spieler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>litern</td>\n",
       "      <td>krise</td>\n",
       "      <td>stehenden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fahrzeuge</td>\n",
       "      <td>stehenden</td>\n",
       "      <td>minuten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hersteller</td>\n",
       "      <td>unten</td>\n",
       "      <td>bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>toyota</td>\n",
       "      <td>pandemie</td>\n",
       "      <td>unten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>porsche</td>\n",
       "      <td>finanzkrise</td>\n",
       "      <td>wm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>modell</td>\n",
       "      <td>quartal</td>\n",
       "      <td>team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diesel</td>\n",
       "      <td>gm</td>\n",
       "      <td>tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>verbrauch</td>\n",
       "      <td>bahn</td>\n",
       "      <td>polizei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>golf</td>\n",
       "      <td>artikel</td>\n",
       "      <td>pandemie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>modelle</td>\n",
       "      <td>porsche</td>\n",
       "      <td>artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coupé</td>\n",
       "      <td>geld</td>\n",
       "      <td>vfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ford</td>\n",
       "      <td>wirtschaftskrise</td>\n",
       "      <td>tor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>peugeot</td>\n",
       "      <td>wirtschaft</td>\n",
       "      <td>michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adac</td>\n",
       "      <td>ubs</td>\n",
       "      <td>gastgeber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kilometer</td>\n",
       "      <td>rezession</td>\n",
       "      <td>löw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fiat</td>\n",
       "      <td>corona</td>\n",
       "      <td>partie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword/automobil_50k.txt keyword/wirtschaft_50k.txt keyword/sport_50k.txt\n",
       "0                         ps                       bank               trainer\n",
       "1                      liter                     dollar            mannschaft\n",
       "2                        bmw                     banken                saison\n",
       "3                       audi                    polizei                    sv\n",
       "4                      wagen                       opel             verlinken\n",
       "5                         vw                  verlinken                  sieg\n",
       "6                   mercedes                    konzern            kostenfrei\n",
       "7                      motor                 kostenfrei                  link\n",
       "8                       opel                       link               spieler\n",
       "9                     litern                      krise             stehenden\n",
       "10                 fahrzeuge                  stehenden               minuten\n",
       "11                hersteller                      unten                bayern\n",
       "12                    toyota                   pandemie                 unten\n",
       "13                   porsche                finanzkrise                    wm\n",
       "14                    modell                    quartal                  team\n",
       "15                    diesel                         gm                   tsv\n",
       "16                 verbrauch                       bahn               polizei\n",
       "17                      golf                    artikel              pandemie\n",
       "18                   modelle                    porsche               artikel\n",
       "19                     coupé                       geld                   vfb\n",
       "20                      ford           wirtschaftskrise                   tor\n",
       "21                   peugeot                 wirtschaft               michael\n",
       "22                      adac                        ubs             gastgeber\n",
       "23                 kilometer                  rezession                   löw\n",
       "24                      fiat                     corona                partie"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llk = da.ll_keywords(n=25)\n",
    "pd.DataFrame(llk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
