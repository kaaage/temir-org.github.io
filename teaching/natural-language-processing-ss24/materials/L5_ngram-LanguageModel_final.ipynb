{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2561bd4-363c-429d-8222-eee54f25ee5d",
   "metadata": {},
   "source": [
    "#  N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1161189",
   "metadata": {},
   "source": [
    "## A1 - Linear Interpolation\n",
    "\n",
    "A Corpus for the German language consists of the two sentences:\n",
    "\n",
    "            (1) Johann gibt Marie das Manuskript.\n",
    "            (2) Peter sieht, dass Johann Marie das Buch gibt.\n",
    "\n",
    "Calculate the probabilities of the sentences *\"Johann gibt Marie das Buch.\"* and *\"Peter sieht das Buch.\"*\n",
    "\n",
    "**(a)** for a \"classic\" **trigram** model trained on sentences (1)-(2). For this excercise, we will ***ignore punctuation*** and only focus on actual words. \n",
    "\n",
    "**(b)** like in (a), but using simple linear interpolation for the trigrams, with the following weighting factors when smoothing:\n",
    "\n",
    "$\\lambda_3 = 0.80$ (Trigrams)<br>\n",
    "$\\lambda_2 = 0.15$ (Bigrams)<br>\n",
    "$\\lambda_1 = 0.05$ (Unigrams)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346cb73-7c9d-46de-87cc-ce95af3c0a20",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#tokens = 13 (punctuation ignored)\n",
    "\n",
    "Unigramm:\n",
    "    \n",
    "$P(Johann) = \\frac{2}{13}$<br>\n",
    "$P(gibt) = \\frac{2}{13}$<br>\n",
    "$P(Marie) = \\frac{2}{13}$<br>\n",
    "$P(das) = \\frac{2}{13}$<br>\n",
    "$P(Manuskript) = \\frac{1}{13}$<br>\n",
    "$P(Buch) = \\frac{1}{13}$<br>\n",
    "$P(Peter) = \\frac{1}{13}$<br>\n",
    "$P(sieht) = \\frac{1}{13}$<br>\n",
    "$P(dass) = \\frac{1}{13}$<br>\n",
    "\n",
    "Bigram:\n",
    "\n",
    "$P(gibt|Johann) = \\frac{1}{2}$<br>\n",
    "$P(Marie|gibt) = \\frac{1}{2}$<br>\n",
    "$P(das|Marie) = \\frac{2}{2}$<br>\n",
    "$P(Manuskript|das) = \\frac{1}{2}$<br>\n",
    "\n",
    "$P(sieht|Peter) = \\frac{1}{1}$<br>\n",
    "$P(dass|sieht) = \\frac{1}{1}$<br>\n",
    "$P(Johann|dass) = \\frac{1}{1}$<br>\n",
    "$P(Marie|Johann) = \\frac{1}{2}$<br>\n",
    "$P(Buch|das) = \\frac{1}{2}$<br>\n",
    "$P(gibt|Buch) = \\frac{1}{1}$<br>\n",
    "    \n",
    "Trigramm:\n",
    "\n",
    "$P(Marie|Johann, gibt) = \\frac{1}{1}$<br>\n",
    "$P(das|gibt, Marie) = \\frac{1}{1}$<br>\n",
    "$P(Manuskript|Marie, das) = \\frac{1}{2}$<br>\n",
    "$P(dass|Peter, sieht) = \\frac{1}{1}$<br>\n",
    "$P(Johann|sieht, dass) = \\frac{1}{1}$<br>\n",
    "$P(Marie|dass, Johann) = \\frac{1}{1}$<br>\n",
    "$P(Buch|Marie, das) = \\frac{1}{2}$<br>\n",
    "$P(gibt|das, Buch) = \\frac{1}{1}$<br>\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#### 1) Classic Tri-gram Model\n",
    "    \"Johann gibt Marie das Buch\" \n",
    "\n",
    "$ P(Johann) \\cdot P(gibt|Johann) \\cdot P(Marie| Johann,gibt) \\cdot P(das|gibt,Marie) \\cdot P(Buch|Marie,das) =\\frac{2}{13}\\cdot \\frac{1}{2}\\cdot \\frac{1}{1}\\cdot \\frac{1}{1}\\cdot \\frac{1}{2}\\cdot = \\frac{2}{4\\cdot 13}=0.038$\n",
    "\n",
    "    \"Peter sieht das Buch\"\n",
    "\n",
    "$ P(Peter) \\cdot P(sieht|Peter) \\cdot P(das|Peter, sieht) \\cdot P(Buch|sieht, das) = \\frac{1}{13} \\cdot 1 \\cdot 0 \\cdot 0 = 0$   \n",
    "\n",
    "#### 2) Linear Interpolation\n",
    "\n",
    "    \"Johann gibt Marie das Buch\"\n",
    "\n",
    "$   P(Johann) \\cdot P(gibt|Johann) \\left(\\lambda_1 P(Marie)+ \\lambda_2 P(Marie|gibt) +\\lambda_3 P(Marie|Johann,gibt)\\right)\\cdot \\left(\\lambda_1 P(das)+ \\lambda_2 P(das|Marie) +\\lambda_3 P(das|gibt,Marie)\\right) \\cdot \\left(\\lambda_1 P(Buch)+ \\lambda_2 P(Buch|das) +\\lambda_3 P(Buch|Marie,das)\\right)$\n",
    "\n",
    "= $\\frac{2}{13} \\cdot \\frac{1}{2}  \\cdot \\left(0.05 \\cdot\\frac{2}{13} + 0.15 \\cdot  \\frac{1}{2} + 0.8  \\cdot \\frac{1}{1} \\right) \\cdot \\left(0.05 \\cdot\\frac{2}{13} + 0.15  \\cdot \\frac{2}{2} + 0.8  \\cdot \\frac{1}{1} \\right) \\cdot \\left(0.05 \\cdot\\frac{1}{13} + 0.15  \\cdot \\frac{1}{2} + 0.8  \\cdot \\frac{1}{2} \\right)$\n",
    " = 0.031\n",
    "\n",
    "\n",
    "     \"Peter sieht das Buch\"\n",
    "$ P(Peter) \\cdot P(sieht|Peter) \\cdot (\\lambda_1 P(das) + \\lambda_2 P(das|sieht) + \\lambda_3 P(das|Peter, sieht)) \\cdot (\\lambda_1 P(Buch) + \\lambda_2 P(Buch|das) + \\lambda_3 P(Buch|sieht, das))$\n",
    "\n",
    "= $\\frac{1}{13} \\cdot 1 \\cdot (0.05 \\cdot \\frac{2}{13} + 0.15 \\cdot 0 + 0.8 \\cdot 0) \\cdot (0.05 \\cdot \\frac{1}{13} + 0.15 \\cdot \\frac{1}{2} + 0.8 \\cdot 0) $\n",
    "\n",
    "= $\\frac{1}{13} \\cdot 0.0077 \\cdot 0.0789 = 0.000046733$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83065f9",
   "metadata": {},
   "source": [
    "## A2 - Language Model with Padding\n",
    "\n",
    "For plain language models there is the problem of the beginning of a sentence:\n",
    "The probability that a unigram occurs should not be the same as the probability that it occurs in the beginning of the sentence. To cope with this we can add special padding symbols around the sentence.\n",
    "Now $P((\\text{<s>}, w_i))$ marks the probability that a sentences starts with $ w_i$.\n",
    "\n",
    "For a **bigram** language model we can now replace the first factor in the probability formula by the padded words, from:\n",
    "\n",
    "$P((w_1,w_2,...w_n)) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_2)\\cdot\\dots\\cdot P(w_n|w_{n-1})$\n",
    "\n",
    "to:\n",
    "\n",
    "$P((w_1,w_2,...w_n)) = P(w_1|\\text{<s>}) \\cdot P(w_2|w_1) \\cdot P(w_3|w_2)\\cdot\\dots\\cdot P(w_n|w_{n-1}) \\cdot P(\\text{</s>}|w_n)$\n",
    "    \n",
    "    \n",
    "This better reflects the probability of how likely $w_1$ will occur at the start of a sentence.\n",
    "Same argument holds for words at the end of sentence and `</s>`.\n",
    "\n",
    "    \n",
    "\n",
    "Consider the following toy example (similar to the one from Jurafsky & Martin, 2021):\n",
    "\n",
    "**Training data:**\n",
    "\n",
    "`<s> I am Sam </s>`<br>\n",
    "`<s> Sam I am </s>`<br>\n",
    "`<s> Sam I like </s>`<br>\n",
    "`<s> Sam I do like </s>`<br>\n",
    "`<s> do I like Sam </s>`<br>\n",
    "\n",
    "Assume that we use a bigram language model (without smoothing or interpolation) based on the above training data.\n",
    "\n",
    "1. What is the most probable next word predicted by the model for the following word sequences?<br>\n",
    "    (1) `<s> Sam ...`<br>\n",
    "    (2) `<s> Sam I do ...`<br>\n",
    "    (3) `<s> Sam I am Sam ...`<br>\n",
    "    (4) `<s> do I like ...`<br>\n",
    "\n",
    "2. Which of the following sentences gets assigned the highest probability with this model?\n",
    "\n",
    "    (5) `<s> Sam I do I like </s>`<br>\n",
    "    (6) `<s> Sam I am </s>`<br>\n",
    "    (7) `<s> I do like Sam I am </s>`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620d0c7-42f3-49c1-bb5d-c8068989b1b3",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "Bigram probabilities:\n",
    "$P(Sam|<s>) = \\frac{3}{5}$<br>\n",
    "$P(I|<s>) = \\frac{1}{5}$<br>\n",
    "$P(I|Sam) = \\frac{3}{5}$<br>\n",
    "$P(</s>|Sam) = \\frac{2}{5}$<br>\n",
    "$P(Sam|am) = \\frac{1}{2}$<br>\n",
    "$P(</s>|am) = \\frac{1}{2}$<br>\n",
    "$P(am|I) = \\frac{2}{5}$<br>\n",
    "$P(like|I) = \\frac{2}{5}$<br>\n",
    "$P(do|I) = \\frac{1}{5}$<br>\n",
    "$P(Sam|like) = \\frac{1}{3}$<br>\n",
    "$P(</s>|like) = \\frac{2}{3}$<br>\n",
    "$P(like|do) = \\frac{1}{2}$<br>\n",
    "$P(I|do) = \\frac{1}{2}$<br>\n",
    "\n",
    "**1.** \n",
    "    (1) and (3): “I”. <br>\n",
    "    (2): “I” and “like” are equally probable.<br>\n",
    "    (4): `</s>` <br>\n",
    "\n",
    "\n",
    "**2.** \n",
    "\n",
    "(5): $\\frac{3}{5}\\cdot \\frac{3}{5}\\cdot\\frac{1}{5} \\cdot \\frac{1}{2} \\cdot \\frac{2}{5}\\cdot\\frac{2}{3} = 0.0144$\n",
    "\n",
    "(6): $\\frac{3}{5} \\cdot \\frac{3}{5} \\cdot \\frac{2}{5}\\cdot \\frac{1}{2} = 0.072$\n",
    "\n",
    "(7): $\\frac{1}{5} \\cdot \\frac{1}{5} \\cdot \\frac{1}{2} \\cdot \\frac{1}{3}\\cdot\\frac{3}{5}\\cdot\\frac{2}{5}\\cdot\\frac{1}{2} = 0.0008$\n",
    "\n",
    "Therefore, (6) is the most probable sentence according to our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3124b4",
   "metadata": {},
   "source": [
    "## A3 - Perplexity\n",
    "Consider again the same training data as in **A2** and the same bigram model. Compute the perplexity of\n",
    "\n",
    "`<s> I do like Sam`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d988a-a81c-4a8e-9238-fb4e18c9e0dd",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "The probability of this sequence is \n",
    "\n",
    "$\\frac{1}{5}\\cdot\\frac{1}{5}\\cdot\\frac{1}{2}\\cdot\\frac{1}{3} = \\frac{1}{150} $\n",
    "\n",
    "The perplexity is then \n",
    "\n",
    "$\\sqrt[4]{150} = 3.5$ if you don't count the padding symbol `<s>` as a word of the input sequence.  <br>\n",
    "Otherwise the solution would be $\\sqrt[5]{150} = 2.7$ \n",
    "\n",
    "The decision to count the padding as a part of the input sequence often depends on practical or technical deliberations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa5194",
   "metadata": {},
   "source": [
    "## A4 - Laplace Smoothing\n",
    "Take again the same training data as in **A2**. This time, we use a bigram LM with **Laplace smoothing**.\n",
    "\n",
    "1. Give the following bigram probabilities estimated by this model:\n",
    "\n",
    "    `P(do|<s>), P(do|Sam), P(Sam|<s>), P(Sam|do), P(I|Sam), P(I|do), P(like|I)`\n",
    "    \n",
    "    \n",
    "2. Calculate the probabilities of the following sequences according to this model:<br>\n",
    "    (8) `<s> do Sam I like`<br>\n",
    "    (9) `<s> Sam do I like`<br>\n",
    "    \n",
    "    Which of the two sequences is more probable according to our LM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de618a36-acfc-45c5-a005-904f4cb1f10c",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "1. \n",
    "$P(do|<s>) = \\frac{2}{12}$<br>\n",
    "$P(do|Sam) = \\frac{1}{12}$<br>\n",
    "$P(Sam|<s>) = \\frac{4}{12}$<br>\n",
    "$P(Sam|do) = \\frac{1}{9}$<br>\n",
    "$P(I|Sam) = \\frac{4}{12}$<br>\n",
    "$P(I|do) = \\frac{2}{9}$<br>\n",
    "$P(like|I) = \\frac{3}{12}$<br>\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "(8): $\\frac{2}{12} \\cdot \\frac{1}{9} \\cdot \\frac{4}{12}\\cdot\\frac{3}{12} = 0.0015$ <br>\n",
    "(9): $\\frac{4}{12} \\cdot\\frac{1}{12} \\cdot \\frac{2}{9} \\cdot \\frac{3}{12} = 0.0015$<br>\n",
    "    \n",
    "The two sequences are equally probable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3577944",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "*D. Jurafsky, J. H. Martin: Speech and Language Processing (3rd ed. Draft), https://web.stanford.edu/~jurafsky/slp3/, 2021*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
